% Template for a Computer Science Tripos Part II project dissertation
\documentclass[12pt,a4paper,twoside,openright]{report}
\usepackage[pdfborder={0 0 0}]{hyperref}    % turns references into hyperlinks
\usepackage[margin=25mm]{geometry}  % adjusts page layout
\usepackage{graphicx}  % allows inclusion of PDF, PNG and JPG images
\usepackage{verbatim}
\usepackage{docmute}   % only needed to allow inclusion of proposal.tex
\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows}
\usepackage{multirow}
\usepackage{url}
\usepackage{csquotes}
\usepackage{float}
\usepackage{textcomp}
\usetikzlibrary{positioning}
\usetikzlibrary{arrows.meta}
\usepackage{forest}
\usepackage{pdfpages}

\definecolor{folderbg}{RGB}{255,165,0}
\definecolor{folderborder}{RGB}{110,144,169}

\def\Size{4pt}
\tikzset{
  folder/.pic={
    \filldraw[draw=folderborder,top color=folderbg!50,bottom color=folderbg]
      (-1.05*\Size,0.2\Size+5pt) rectangle ++(.75*\Size,-0.2\Size-5pt);  
    \filldraw[draw=folderborder,top color=folderbg!50,bottom color=folderbg]
      (-1.15*\Size,-\Size) rectangle (1.15*\Size,\Size);
  }
}
 
 
 
 
 
 
 
 
% Conditionals (for draft mode)
\usepackage{ifthen}

% Color
\usepackage{color}

% Date/time stuff
\usepackage{datetime}

\usepackage{xspace}




%    THIS IS THE BIG DRAFT SWITCH
% FLICK THIS AND the todo notes and watermark disappear/appear


\newboolean{draft}
\setboolean{draft}{false}

\newboolean{comments}

\ifdraft
  \setboolean{comments}{true}
\else
  \setboolean{comments}{false}
\fi

\newcommand\todo[1]{\ifcomments{\color{cyan}{\bf ToDo: #1}}\fi}

\newcommand\note[2]{{\color{#1}\bf #2}}
\newcommand\awm[1]{\ifcomments{\note{blue}{AM: #1}}\fi}
\newcommand\simon[1]{\ifcomments{\note{cyan}{SM: #1}}\fi}
% Watermark in draft mode
% Draft watermark
\ifdraft
\usepackage{calc} 
\usepackage{ifthen} 
\newcounter{hours}\newcounter{minutes} 
\newcommand\printtime{% 
  \setcounter{hours}{\time/60}% 
  \setcounter{minutes}{\time-\value{hours}*60}% 
  \ifthenelse{\value{hours}<10}{0\thehours}{\thehours}:\hspace{-0.33em} 
  \ifthenelse{\value{minutes}<10}{0\theminutes}{\theminutes} 
} 

 \usepackage{draftwatermark}
 \SetWatermarkScale{1.1}
 \SetWatermarkLightness{0.9}
 \SetWatermarkText{\shortstack{DRAFT  \\[0.5cm] {\fontsize{38}{12}\selectfont 
         %\selectlanguage{<language>}
         \today, {\currenttime}}%, \printtime
     }}
  \fi


\raggedbottom                           % try to avoid widows and orphans
\sloppy
\clubpenalty1000%
\widowpenalty1000%

\renewcommand{\baselinestretch}{1.1}    % adjust line spacing to make
                                        % more readable

\begin{document}

\bibliographystyle{plain}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Title

%TC:ignore
\pagestyle{empty}

\rightline{\LARGE \textbf{Simonas Mulevicius}}
% Blind Grading Number == 2439D

\vspace*{60mm}
\begin{center}
\Huge
\textbf{Analysing the behaviour and performance of \texttt{QUIC}} \\[5mm]
Computer Science Tripos -- Part II \\[5mm]
Homerton College \\[5mm]
\today  % today's date
\end{center}








\newpage



\section*{Declaration of Originality}

I, Simonas Mulevicius of Homerton College, being a candidate for Part II of the Computer
Science Tripos, hereby declare
that this dissertation and the work described in it are my own work,
unaided except as may be specified below, and that the dissertation
does not contain material that has already been used to any substantial
extent for a comparable purpose. I am content for my dissertation to
be made available to the students and staff of the University.

\bigskip
\leftline{Signed [signature]}

\medskip
\leftline{Date [date]}


\newpage










%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Proforma, table of contents and list of figures

\pagestyle{plain}

\chapter*{Proforma}

{\large
\begin{tabular}{ll}
Candidate Number:               & \bf 2439D                      \\
College:            & \bf Homerton College                     \\
Project Title:      & \bf \texttt{QUIC} offloading using NetFPGA smart NICs \\
Examination:        & \bf Computer Science Tripos -- Part II, July 2021  \\
Word Count:         & \bf 11874\footnotemark[1] \\
Line Count:         & \bf 0 \simon{TODO: calculate} \\
Project Originator: & Professor Andrew W. Moore                \\
Supervisor:         & Professor Andrew W. Moore                \\ 
\end{tabular}
}
\footnotetext[1]{This word count was computed
by \texttt{texcount diss.tex}. In addition, \texttt{\%TC:ignore} and \texttt{\%TC:endignore} comments were added to ignore all but the five main sections}

\stepcounter{footnote}


\section*{Original Aims of the Project}

The initial primary goal of the project was to offload packet reordering logic of \texttt{QUIC} to hardware.
However, because of the increased project complexity due to the COVID-19 leading to an unexpectedly long preparation phase, difficulties in system access, and challenges with the \texttt{NetFPGA SUME} boards, I changed the primary focus of the project.
Hence, the adjusted goals of this dissertation are to study the behaviour of \texttt{QUIC} under different network conditions, to measure the impact of network perturbations for the throughput of \texttt{QUIC}, and identify bottleneck in \texttt{QUIC} implementations.

\section*{Work Completed}

I configured a network emulation and testing environment allowing the performance measurements of \texttt{QUIC}.
Then, I measured \texttt{QUIC} throughput under different network conditions, e.g., when I applied additional packet delay, packet loss, and packet reordering.

\section*{Special Difficulties}

The project's initial goal was to write hardware components in \texttt{Verilog} that would offload packet reordering logic of \texttt{QUIC} packets to hardware.
For this project, I had access to two dedicated experimental machines equipped with specialised hardware components -- \texttt{NetFPGA SUME} network interface cards.
However, the software and hardware stack of \texttt{NetFPGA} platform was relatively complex.
For instance, it took me unexpectedly long to prepare, configure and test the correct functionality of \texttt{NetFPGA SUME} boards because, as I have already mentioned, COVID-19 complicated the set-up process.

To simplify the development process, I had a pair of the aforementioned machines in my college room.
However, I faced accessibility issues during the winter vacation when I was abroad at home.
In particular, the testing machines were expensive, heavy and fragile, so I could not take them with me for the winter vacation.
Hence, to overcome this issue, I had to configure remote access to the testing environment.
I spent more than one month working on the remote set-up of both the main pair of testing machines as well as the backup pair.
Unfortunately, I could not configure two additional backup machines in the Computer Laboratory.
A suspected issue was a possible hardware fault.
Hence, to continue working on the project, I had to return to my college room.
This lengthy preparation phase substantially delayed the actual development of the project.
As a result, after consultation with the project supervisor, I decided to change the focus of the dissertation from hardware offloading to the software-oriented performance analysis of \texttt{QUIC}. 
In conclusion, one could argue that in pre-pandemic times, physical access to the pre-configured and tested hardware could have saved me a lot of time.


%Professional Practice and Presentation 14%
%Introduction and Preparation 26%
%Implementation 40%
%Evaluation and Conclusion 20%




\tableofcontents

\listoffigures

\newpage
\section*{Acknowledgements}

The following people helped with the project setup or provided useful insights or hints on how to tackle project-related problems:
\begin{itemize}
    \item \textbf{Dr Marcin Wojcik} helped with general inquiries related to the setup of NetFPGA developing platform;
    \item \textbf{Professor Andrew W. Moore} was my project supervisor and originator of the project idea;
    \item \textbf{Dr Malcolm Scott} helped with the configuration of two remote backup machines in the Computer Laboratory during the winter vacation;
    \item \textbf{Mr Tatsuhiro Tsujikawa} is the author of the \texttt{ngtcp2} implementation of \texttt{QUIC}, and he answered usability questions related to it. Moreover, he provided guidance on how \textit{null} encryption could be turned on in \texttt{ngtcp2};
    \item \textbf{Mr Nick Banks}, software developer from \texttt{Microsoft}, is the co-author of the \texttt{MsQuic} implementation of \texttt{QUIC}, and he helped with the setup of this particular implementation of \texttt{QUIC};
    \item Part II students \textbf{Ms Akvile Valentukonyte} and \textbf{Mr Justas Janickas} participated in our weekly meetings where we all shared our achieved progress;
\end{itemize}

Furthermore, this document is written using a default dissertation template provided by Martin Richards~\cite{how_to_write_a_dissertation_in_LATEX}.
However, some formatting ideas are taken from Alex Coplan's dissertation~\cite{Alex_Coplan_dissertation}, which is \enquote{highly commended by the examiners}~\cite{Computer_Lab_dissertations}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% now for the chapters

\pagestyle{headings}

%TC:endignore
\chapter{Introduction}

% ------------------------------------------------
% TODO
%
% The introduction should explain the principal motivation for the project and show how the work fits into the broad area of surrounding computer science and give a brief survey of previous related work. It should generally be unnecessary to quote at length from technical papers or textbooks. If a simple bibliographic reference is insufficient, consign any lengthy quotation to an appendix.



% FROM: https://www.cst.cam.ac.uk/teaching/part-ii/projects/assessment
% Clear motivation, justifying potential benefits of success.
%Good or excellent requirements analysis; justified and documented selection of suitable tools; good engineering approach.
%Clear presentation of challenging background material covering a range of computer science topics beyond Part IB.
% ------------------------------------------------



% TODO: say what the problem is
% > explain the problem
% > why is the problem interesting?
% > why does the solution enable?

% TODO: describe my work
% > how does my work solve the problem (high-level overview)?
% > details in other chapters

\awm{(this paragraph is the abstract\\
\textit{What is the PROBLEM?}\\
New HTTP protocols need to be well understood for performance and behaviour - the newest of these is QUIC...\\
\textit{Why do people care?}\\
the web is the Internet, yet earlier HTTP does not have the rich features people now desire (you might enumerate a short list of things the HTTP and TCP protocols don't do); QUIC provides many features, but as a new protocol is far from the high-performance transport we have come to need and expect in the Internet \\
\textit{What are you doing/did you do?}\\
Experimental study of a QUIC implementation comparing the behaviour of two implementations and identifying core contributors for future improvement and optimization 
\textit{why is it awesome?}\\
I show that while cryptographic functions are a significant overhead in the current QUIC implementations and hardware offload can make a significant improvement, even using a null-cryptographic routine, the performance of QUIC falls far below its potential and significantly below comparable TCP performance.
}

% TODO add reference to the ideas of this paragraph:
Nowadays, more and more people have access to the Internet.
As the scale of the web continues to grow, both companies and researchers look for ways to satisfy ever-increasing market demand to improve the user experience of Internet users.
Customers expect cheap, high-throughput and low latency Internet connections.
However, in the last decade, several trends have emerged.
For example, in recent years, mobile devices proliferated -- at the time of writing, around $92.6\%$ of Internet users used mobile devices to connect to the Internet~\cite{bib_number_of_mobile_users}.
Furthermore, there are emerging new patterns in web development.
In particular, usual websites nowadays contain multiple embedded objects of various sizes (e.g. from \texttt{JavaScript} or \texttt{CSS} files to numerous images and videos)\cite{bib_Netdev_0x13_QUIC_Tutorial}.
But, these websites look entirely different from the simple, single document \texttt{HTML} websites used in the early days of the Internet~\cite{RudmanRiaan2016DW3o}.
Hence, the changing constraints and networking trends encourage us to re-evaluate past protocols.
Innovative solutions can further improve the performance of the Internet. 
One of such ideas is \texttt{QUIC} -- the new transport layer protocol.

\section{The motivation of the Project}

The purpose of this dissertation is to understand \texttt{QUIC}.
As I describe in the later chapters, this innovative transport layer protocol offers both performance and privacy improvements.
However, \texttt{QUIC} has many new features that are either not present in \texttt{TCP} or that are used in different ways.
Hence, because of this novelty, the performance behaviour of \texttt{QUIC} is usually not well understood.
Thus, in this project, I seek to understand several key features of \texttt{QUIC} that determine its performance.
Then, I analyse existing implementations of \texttt{QUIC} aiming to identify their performance bottlenecks.
Finally, this analysis could provide guidance for further \texttt{QUIC} improvements that could lead to the most immediate effects.


\section{Objectives of the Project}

The primary goal of this project is to identify the bottlenecks of \texttt{QUIC} and characterise its performance behaviour.
\texttt{QUIC} transport layer protocol is a new protocol.
However, it is already necessary to start thinking about its performance.
Otherwise, the low throughput of \texttt{QUIC} may prevent it from being widely adopted (as happened with earlier attempts to improve \texttt{TCP}). 
% TODO: add a reference to the source proving this claim


% ----------------------------------------------
% Removed according to Andrew's recommendations:
% ----------------------------------------------
% <START_OF_REMOVAL>
%The most interesting part of this project is that  \texttt{QUIC} is a relatively new network technology and, as far as I am aware, there have not been any similar projects which tried to measure \texttt{QUIC}’s performance by \enquote{breaking} it (i.e. using \texttt{QUIC} without encryption).
% TODO
%[TODO: incorrect - Mr  Nick  Banks proposed null-crypto option already in autumn of 2020 but I was not aware of that]

% This analysis could be useful in the future when \texttt{QUIC}’s cryptographic operations would be offloaded to hardware. Nevertheless, these experiments could also have some tangible value for current times. 
% For instance, an experimental \texttt{QUIC} with \textit{null} encryption could be used in secure networks controlled by a single entity (e.g. within a data centre).
% Alternatively, unencrypted \texttt{QUIC} traffic could be used to isolate expensive cryptographic operations from the performance analysis of \texttt{QUIC}, as suggested in~\cite{banks-quic-disable-encryption-00}.
% <END_OF_REMOVAL>
% ----------------------------------------------





%-------IDEAS to add to this section:-------


%The performance study of \texttt{QUIC} under several variable conditions (e.g. packet loss, reordering and delay) in comparison with iperf is the focus of my work.
 
 
%The purpose of this work is to predict future bottlenecks. To achieve this, I would first need to exclude/bypass the cryptographic operations of  \texttt{QUIC}. I already have the functionality to turn on null-encryption on the server-side, so I would need to adopt similar ideas for the  \texttt{QUIC} client. Such a set-up would simulate offloaded encryption. Previous studies analysed different implementations of  \texttt{QUIC} (not ngtcp2), so the primary goal would be to run similar experiments with ngtcp2. After that, I would have to identify situations where  \texttt{QUIC} (ngtcp2 in particular) outperforms TCP using different network conditions. To achieve this, I have a complete testing environment with a configured intermediate machine that can introduce traffic perturbations. In addition, I have already performed some performance measurements using different MTUs. Furthermore, I estimated the impact of core pinning for these tests and took into account both hyperthreading and GSO.
 
 

\subsection{High-Level Overview of \texttt{QUIC}}
I detail \texttt{QUIC} in the later sections of the dissertation (see section~\ref{QUIC_details_section}) but, briefly speaking, \texttt{QUIC} is a new transport layer protocol that combines ideas of \texttt{TCP}, \texttt{HTTP} and other experimental protocols such as \texttt{SPDY} (which is also briefly mentioned in the later sections -- see \ref{Previous_attempt_to_improve_http_by_using_SPDY_and_HTTP2}).
On the one hand, \texttt{QUIC} breaks the separation of transport and security layer protocols. 
On the other hand, \texttt{QUIC} protocol offers some performance benefits for modern-day traffic flows (see section~\ref{QUIC_features}).


\subsection{Importance of Performance Measurements of \texttt{QUIC}}

\texttt{QUIC}, after being publicly launched in 2013~\cite{Chromium_Blog_Experimenting_with_quic},  is still a relatively new protocol that is still in the development stage.
Hence, any performance measurements of \texttt{QUIC} guide the ongoing efforts to improve this new transport layer protocol.
Furthermore, there are strong indications showing that \texttt{QUIC} could be widely adopted in the future.
For instance, \texttt{Google}, one of the major advocates for \texttt{QUIC}, operates both on the server side (e.g. search engine or video streaming service \texttt{YouTube}) and the client side (e.g. \texttt{Google Chrome} browser for desktop and mobile devices)~\cite{A_QUICk_Introduction_to_HTTP3}.
Hence, using its existing market presence \texttt{Google} could potentially encourage the faster adoption of \texttt{QUIC}.
Actually, according to the report published in 2016~\cite{RuthJan2018AFLa}, at that time, around 40\% of all traffic of \texttt{Google} was using \texttt{QUIC}.
Similarly, other large technology companies such as \texttt{Facebook} or \texttt{Microsoft} are also building their own implementations of \texttt{QUIC} (correspondingly \texttt{mvfst}\footnote{https://github.com/facebookincubator/mvfst} and \texttt{MsQuic}\footnote{https://github.com/microsoft/msquic}).
These companies are also pushing the deployment of \texttt{QUIC}. 
For example, according to the report from \texttt{Facebook} published in 2020~\cite{how-facebook-is-bringing-quic-to-billions}, \texttt{QUIC} was used for the three quarters of \texttt{Facebook} traffic. 
In conclusion, it is highly likely that \texttt{QUIC} would get a wide range of support, meaning that current performance measurements of \texttt{QUIC} could help shape the future.

\section{Related Work}
There have been several similar attempts to measure \texttt{QUIC} performance under different network scenarios. 
Yang et al.~\cite{Making_QUIC_Quicker} analysed the impact of network delay, packet reordering and packet delay for the \texttt{QUIC} throughput; the authors examine several different implementations of \texttt{QUIC} written in the C/C\texttt{++} language family.
Their performance analysis of \texttt{QUIC} used two machines -- one hosted both the client and the server while the second machine introduced controlled network perturbations.
As illustrated in Figure~\ref{fig:Physical_testing_environment}, I also used this setup for my performance analysis of \texttt{QUIC}.
In addition, Yang et al.~\cite{Making_QUIC_Quicker} show that slight packet reordering can substantially decrease the throughput of \texttt{QUIC} .
They then suggest that one reason for such behaviour is that reordered packets can be treated as lost packets~\cite{Making_QUIC_Quicker}. 


In order to measure the impact of cryptography for \texttt{QUIC} performance, I extended one implementation of \texttt{QUIC} to support \enquote{Null-encryption}.
However, later I discovered that the option to disable encryption of \texttt{QUIC} was already proposed~\cite{banks-quic-disable-encryption-00}.
The author noted in private correspondence that this ideas had been already implemented in \texttt{MsQuic}, the aforementioned \texttt{Microsoft}'s implementation of \texttt{QUIC}.



An analysis of the \texttt{QUIC} protocol was published in 2018~\cite{overview_of_the_QUIC_protocol}.
We both use the same implementation of \texttt{QUIC}, called \texttt{ngtcp2}.
Indeed, \texttt{ngtcp2} has a well-documented code-base with several examples, e.g., client and server implementations.   
However, \texttt{QUIC} in 2018 was still in the early development stage as it had only been publically published for several years~\cite{Chromium_Blog_Experimenting_with_quic}.
Sanadhi~\cite{overview_of_the_QUIC_protocol} focused on the correctness of \texttt{ngtcp2} and another implementation of \texttt{QUIC} called \texttt{gQUIC}.
They inspected the correct order of packet exchange, although no quantitative performance analysis of the protocol was performed.
Nevertheless, in the \enquote{Future work} chapter, the author suggests carrying out a performance analysis of \texttt{QUIC} under different simulated network conditions.
As a result, my work could be viewed as the continuation of the previous analysis of \texttt{QUIC}~\cite{overview_of_the_QUIC_protocol}.




% TODO: mention the work of \texttt{QUIC} performance measurements.
% https://dl.acm.org/doi/pdf/10.1145/3242102.3242106



\section{Completed Work}
My analysis shows that a \texttt{QUIC} client can spend around 70\% of CPU time performing cryptographic operations. 
My results are consistent with previous studies that identified cryptographic functions as the current bottleneck of \texttt{QUIC}~\cite{Making_QUIC_Quicker}.






% -------------------------------
% Summary of https://dl.acm.org/doi/pdf/10.1145/3242102.3242106
% "Implementation and Performance Evaluation of the \texttt{QUIC} Protocol in Linux Kernel"

% one of the major points of \texttt{QUIC} is HTTPS <idea taken from Abstract of this paper> -- because HTTPS includes crypto and \texttt{QUIC} can combine the handshakes of both transport and sec layers

% idea of this paper is that the authors implemented \texttt{QUIC} in kernel (not user space, which is a common architectural decision for \texttt{QUIC} but not TCP).
% then, they compared performance of TCP and \texttt{QUIC} when they both ran in kernel space

% The paper links to other performance measurements

% The paper also uses different RTTs and packet loss rates

% The paper used instantaneous throughput to demonstrate the the results!!!

% The paper used 10ms RTT for baseline measurements 

% The paper also discovered that 0.1% packet drop ratio reduced throughput substantially

% The paper also has throughput/(packet-loss-rate) graph
% -------------------------------



% \section{Number of words}
% TODO TO USE

% An approximate word count of the body of the dissertation may be
% obtained using:

% \texttt{wc diss.tex}

% \noindent
% Alternatively, try something like:
% \verb/detex diss.tex | tr -cd '0-9A-Z a-z\n' | wc -w/





\chapter{Preparation}
% ------------------------------------------------
% TODO
%
% Principally, this chapter should describe the work which was undertaken before code was written, hardware built or theories worked on. It should show how the project proposal was further refined and clarified, so that the implementation stage could go smoothly rather than by trial and error.

%Throughout this chapter and indeed the whole dissertation, it is essential to demonstrate that a proper professional approach was employed.

%The nature of this chapter will vary greatly from one dissertation to another but, underlining the professional approach, this chapter will very likely include a section headed "Requirements Analysis" and refer to appropriate software engineering techniques used in the dissertation. The chapter will also cite any new programming languages and systems which had to be learnt and will mention complicated theories or algorithms which required understanding.

%It is essential to declare the starting point. This states any existing codebase or materials that your project builds on. The text here can commonly be identical to the text in your proposal, but it may enlarge on it or report variations. For instance, the true starting point may have turned out to be different from that declared in the proposal and such discrepancies must be explained.



% FROM: https://www.cst.cam.ac.uk/teaching/part-ii/projects/assessment
% Clear motivation, justifying potential benefits of success.
%Good or excellent requirements analysis; justified and documented selection of suitable tools; good engineering approach.
%Clear presentation of challenging background material covering a range of computer science topics beyond Part IB.





% TODO: 1. explain the any background knowledge needed
% TODO: 2. explain the techniques and tools I will use
% TODO: 3. explain the practices I planned to use

% Background knowledge
% Requirements analysis
% > Which techniques and tools coul I use?
% > What did I actually decide to use?
% > Which factors led me to these decisions?
% Methods and Tools
% > what are the engineering practices me project followed?
% > > software -- testing and source control
% > > experiments
% > compare my practices with good engineering practices and explain how/why I decided to do that?

% ------------------------------------------------

In this chapter, I provide background knowledge about the networking landscape of \texttt{QUIC}. 
Then I analyse \texttt{QUIC} protocol in more detail.
Secondly, I present my requirements analysis.
Finally, I describe software engineering tools and techniques used during this project.


\section{Networking Landscape Before \texttt{QUIC}} \label{Networking_Landscape_Before_QUIC}
    \texttt{QUIC} aims to solve some of the design problems present in the \texttt{TCP} protocol. The hope is that improvements in underlying transport layer protocols would improve the application layer protocols' performance.
    In particular, the primary focus is to reduce latency and increase the throughput of the \texttt{HTTP} protocol.
    Hence, it is worth studying previous attempts to improve the performance of HTTP.


\subsection{\texttt{HTTP/1.0} Problems Caused by \texttt{TCP}}
    \texttt{HTTP/1.0} suffers from using one \texttt{TCP} connection for every \texttt{HTTP} request-reply pair \cite{TCP_IP_Guide_Book, HTTP_3_the_past_the_present_and_the_future} --
    opening a new \texttt{TCP} connection for every web object causes \texttt{TCP} to remain in its \enquote{slow start} phase, thus small files are rarely sent at peak throughput~\cite{HTTP_3_the_past_the_present_and_the_future}.
    Furthermore, \texttt{HTTPS} suffers from every request needing to complete both \texttt{TCP} and \texttt{TLS} handshakes that take several round-trip times to complete~\cite{HTTP_3_the_past_the_present_and_the_future}.
    Even though, HTTP/1.0 was a suitable solution for the early days of the Internet when all the HTML files were self-contained~\cite{TCP_IP_Guide_Book_2}, it is not efficient to open and close a new \texttt{TCP} connection for each of the multiple embedded web objects associated with standard websites.

\subsection{Previous Attempt to Improve \texttt{HTTP} by Using \texttt{HTTP/1.1}}

According to the initial draft of \texttt{HTTP/1.1}~\cite{RFC2068}, \texttt{HTTP/1.1} was proposed to mitigate some of the \texttt{HTTP/1.0} problems by introducing persistent connections.
This in turn allows multiple \texttt{HTTP} requests to share the same \texttt{TCP} connection.
Besides being beneficial for small objects, the advantage of using a shared \texttt{TCP} connection is that\texttt{TCP} can make better estimates of the actual round-trip time (\texttt{RTT}) between the server and the client~\cite{bib_Computer_Networking_L6}.
More precise \texttt{RTT} measurements allow more consistent utilisation of networking resources.
For example, by evaluating \texttt{TCP} parameters, such as retransmission-timeout, more accurately, the number of unnecessary pre-emptive \texttt{TCP} timeouts can be reduced~\cite{bib_rtt_tcp_Retransmissions}.

Furthermore, \texttt{HTTP/1.1} enables pipelining of multiple \texttt{HTTP} requests over the same \texttt{TCP} connection.
Hence, as stated in~\cite{bib_digital_ocean_http11_vs_http2}, the client does not need to wait for an \texttt{HTTP} reply from the server before sending consecutive \texttt{HTTP} requests.
However, the problem with \texttt{HTTP/1.1} is that \texttt{HTTP} replies need to be sent in order~\cite{RFC7540}. 
So a missing packet of one \texttt{HTTP} request-reply pair could block subsequent \texttt{HTTP} replies~\cite{bib_digital_ocean_http11_vs_http2, head-of-line-blocking-in-quic-and-http-3-the-details}.
In the literature, this issue is called Head-of-line blocking (abbreviated HOL).
As Robin Marx wrote, HOL could have a detrimental effect on user experience when sequentially rendering web pages that have dynamically generated content or that contain large embedded objects~\cite{head-of-line-blocking-in-quic-and-http-3-the-details}.



One possible workaround to hide the effect of Head-of-line blocking is to use several shared \texttt{TCP} connections~\cite{bib_digital_ocean_http11_vs_http2}.
In particular, \texttt{HTTP/1.1} uses six \texttt{TCP} connections ~\cite{bib_will-http2-make-my-site-faster, head-of-line-blocking-in-quic-and-http-3-the-details}.
But, the use of a large number of \texttt{TCP} connections is not practical because there is an associated performance overhead of opening and using them ~\cite{bib_digital_ocean_http11_vs_http2, head-of-line-blocking-in-quic-and-http-3-the-details}.
However, servers also have to share some of the overheads.
For instance, additional \texttt{TCP} connections may require additional memory to store the associated \texttt{TCP} parameters on the server-side~\cite{head-of-line-blocking-in-quic-and-http-3-the-details}.
Finally, application layer Head-of-line blocking problems of \texttt{HTTP/1.1} could be summarised in Figure~\ref{fig:Head_of_line_blocking_of_HTTP1_1}.
This diagram demonstrates that \texttt{HTTP/1.1} fails to address the Head-of-line blocking problem if the server has to transfer more web objects than there are open \texttt{TCP} connections~\cite{head-of-line-blocking-in-quic-and-http-3-the-details}.
For example, the transfer of files 3 and 8 can not be started until the blocking resources (i.e. files 2 and 7) are sent. 

    \begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{figs/Head_of_line_blocking_of_HTTP1_1.png}
    \caption[Head-of-line blocking of \texttt{HTTP/1.1}]{Head-of-line blocking of \texttt{HTTP/1.1}. This diagram visualises the ideas presented in Robin Marx's description of the \texttt{HTTP/1.1} HOL problem~\cite{head-of-line-blocking-in-quic-and-http-3-the-details}}
    \label{fig:Head_of_line_blocking_of_HTTP1_1}
    \end{figure}





\subsection{Previous Attempt to Improve \texttt{HTTP} by Using \texttt{SPDY} and \texttt{HTTP/2.0}} \label{Previous_attempt_to_improve_http_by_using_SPDY_and_HTTP2}
%[TODO describe \texttt{SPDY}]

In order to improve \texttt{HTTP} performance, Google developed a session layer protocol called \texttt{SPDY}~\cite{bib_SPDY_white_paper}.
Then, now obsolete \texttt{SPDY} protocol was used as a blueprint when building the next-generation version of \texttt{HTTP} called \texttt{HTTP/2.0}~\cite{bib_SPDY_vs_HTTP2}.
As Ilya Grigorik states in his book \cite[Chapter~12]{bib_grigorik2013}, the primary achievements of \texttt{HTTP/2.0} are that, compared with \texttt{HTTP/1.1}, \texttt{HTTP/2.0} improves throughput and reduces latency of \texttt{HTTP} protocol by improving utilisation of existing resources.
For example, \texttt{HTTP/2.0} introduced compression of header fields thus reducing communication overhead of \texttt{HTTP} \cite[Chapter~12]{bib_grigorik2013}.
However, the major innovation of \texttt{HTTP/2.0} over \texttt{HTTP/1.1} is that \texttt{HTTP/2.0} improved resource multiplexing \cite[Chapter~12]{bib_grigorik2013}.
Instead of using six \texttt{TCP} connections to hide the problem of Head-of-line blocking, \texttt{HTTP/2.0} uses a single \texttt{TCP} connection~\cite{bib_grigorik2013, head-of-line-blocking-in-quic-and-http-3-the-details}.
As stated in Chapter 12 of Ilya Grigorik's book~\cite{bib_grigorik2013}, to solve the HOL problem at the application level, \texttt{HTTP/2.0} shares the aforementioned single \texttt{TCP} connection over the varying number of streams.
Ideas of \texttt{HTTP/2.0} multiplexing are visualised in Figure~\ref{fig:Multiplexing_HTTP2}.
This diagram demonstrates that logical and independent streams are implemented using frames~\cite{head-of-line-blocking-in-quic-and-http-3-the-details}.
In particular, a single \texttt{TCP} packet may contain several frame headers with their associated payloads~\cite{head-of-line-blocking-in-quic-and-http-3-the-details}.
These \texttt{HTTP/2.0} frame headers allow the protocol to identify different resources and reassemble them independently from each other on the client-side.

    \begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figs/Multiplexing_HTTP2.png}
    \caption[Multiplexing of \texttt{HTTP/2.0} using logical streams and frames]{Multiplexing of \texttt{HTTP/2.0} using logical streams and frames. This diagram combines ideas from the Sutandi's Master project about \texttt{QUIC}
   ~\cite{overview_of_the_QUIC_protocol}
    as well as the article was written by Robin Marx~\cite{head-of-line-blocking-in-quic-and-http-3-the-details}}
    \label{fig:Multiplexing_HTTP2}
    \end{figure}
    
    
    
    
    


\texttt{HTTP/2.0} solves the initial problem of \texttt{HTTP} Head-of-line blocking, but \texttt{TCP} still suffers from the Head-of-line blocking induced by the \texttt{TCP} itself~\cite{bib_making_web_faster_with_http2, bib_TCP_Head_of_line_blocking_stackoverflow, How-does-HTTP-2-solve-the-Head-of-Line-blocking-HOL-issue, head-of-line-blocking-in-quic-and-http-3-the-details}.
In other words, if a \texttt{TCP} packet containing frames of one stream is lost, then all the remaining \texttt{TCP} packets containing frames of other streams can not be delivered to the application layer until the missing packet is not retransmitted~\cite{bib_making_web_faster_with_http2, bib_TCP_Head_of_line_blocking_stackoverflow, How-does-HTTP-2-solve-the-Head-of-Line-blocking-HOL-issue}.
The reason for this is that \texttt{TCP} is a stream-oriented protocol, and it guarantees in-order data delivery.
Furthermore, the underlying \texttt{TCP} transport layer is unaware of its byte-stream structure~\cite{head-of-line-blocking-in-quic-and-http-3-the-details}.
As a result, \texttt{TCP} does not try to infer the boundaries of different \texttt{HTTP/2.0} frames~\cite{head-of-line-blocking-in-quic-and-http-3-the-details}.
Hence, in the presence of packet loss, \texttt{TCP} would not deliver to the above layer the correctly-received in-order \texttt{HTTP} frames that were received in the subsequent \texttt{TCP} packets from different logical \texttt{HTTP} streams~\cite{head-of-line-blocking-in-quic-and-http-3-the-details}.
Even correctly delivered \texttt{HTTP} frames would not be delivered until the next correct retransmission of the missing previous \texttt{TCP} packets~\cite{head-of-line-blocking-in-quic-and-http-3-the-details}.
\texttt{TCP} induced head-of-line blocking has other side effects.
As Gaetano Carlucci et al.~\cite{HTTP_over_UDP_An_Experimental_Investigation_of_QUIC}, the lost \texttt{TCP} packet would reduce the congestion window for all streams multiplexed over the same \texttt{TCP} connection.



% TODO
%[MAYBE TODO: mention Server push functionality]
%[TODO: what is the difference between \texttt{SPDY} and HTTP/2?]


\subsection{Historical Reasons for Invention of \texttt{QUIC}}
As demonstrated in the previous sections, there have already been several attempts to overcome the Head-of-line blocking problems present in \texttt{HTTP}.
In order to finally resolve the HOL blocking problem of \texttt{TCP}, one would need to upgrade this protocol so that \texttt{TCP} could separate frames associated with different logical \texttt{HTTP} streams~\cite{head-of-line-blocking-in-quic-and-http-3-the-details}.
Hence, a new version of the \texttt{TCP} algorithm would be required. 
However, attempts to roll out experimental versions of \texttt{TCP} to yield benefits to the protocols in the application layer were mostly unsuccessful~\cite{bib_Netdev_0x13_QUIC_Tutorial, PollardBarry2019HiAP}.
For instance, there was a largely unsuccessful proposal called \texttt{TCP Fast Open} to reduce the number of handshakes (and thus round-trip times) required to open \texttt{TCP} connections~\cite{the-sad-story-of-tcp-fast-open}.
One of the failure factors was related to middleboxes~\cite{the-sad-story-of-tcp-fast-open}.
During the early 2000s, a specific architectural problem emerged.
Around that time, the world faced an issue that the Internet was running out of \texttt{IPv4} addresses.
Hence, during that period, middleboxes (e.g. Network Address Translators or NATs for short) gained popularity because it was a temporary and cost-effective solution to the shortage of IP addresses~\cite{MurphyNiallRichard2005Ina, bib_Netdev_0x13_QUIC_Tutorial}.
However, newly introduced middleboxes started to interfere with the \texttt{TCP} traffic~\cite{bib_Netdev_0x13_QUIC_Tutorial, PollardBarry2019HiAP}.
For instance, NATs made assumptions about the structure of \texttt{TCP} headers~\cite{bib_Netdev_0x13_QUIC_Tutorial, PollardBarry2019HiAP}.
As \texttt{TCP/IP} stack was and still is a predominant backbone of the Internet~\cite{tcp-ip-stack_shtml}, some NATs offloaded some \texttt{TCP} processing to hardware or used some software optimisation techniques.
Consequently, these premature optimisations interfered with \texttt{TCP} protocol's correctness -- NATs rejected connections that used experimental \texttt{TCP} packets~\cite{PollardBarry2019HiAP}.
One reason for the slow roll-out of \texttt{TCP} improvements was that \texttt{TCP's} functionality is implemented in the OS kernel.
To add changes to \texttt{TCP}, one would need to change a large number of different operating systems ~\cite{PollardBarry2019HiAP}.
Hence, it was decided that it would be relatively difficult to deploy upgrades for the \texttt{TCP} transport layer protocol~\cite{head-of-line-blocking-in-quic-and-http-3-the-details}.
However, \texttt{TLS 1.3} was another protocol that suffered from the middlebox interference~\cite{why-tls-1-3-isnt-in-browsers-yet}.

Architects of \texttt{QUIC} learned these mistakes and attempted to prevent the ossification of \texttt{QUIC} in the future by using several interesting techniques.
To simplify the deployment, \texttt{QUIC's} code operates in user space~\cite{quic-and-http-3-too-big-to-fail}.
In addition, to prevent NATs from interfering with \texttt{QUIC}, \texttt{QUIC's} packet headers are partially encrypted (see subsection \ref{subsection_QUIC_header_format})~\cite{bib_Netdev_0x13_QUIC_Tutorial}.
Finally, instead of upgrading \texttt{TCP} or building a completely new transport layer on top of \texttt{IP}, \texttt{QUIC} builds on top of \texttt{UDP}~\cite{chromium_blog_about_quic}.
The main reason for this is that \texttt{UDP} is an already deployed, supported and recognised transport protocol of the Internet~\cite{the-road-to-quic}.

In conclusion, all these enhancements of \texttt{HTTP} can be summarised in the following historical timeline (see Figure~\ref{fig:cf-secure-web-timeline}).
% TODO: reduce the size of this diagram and take only the most important developments
    \begin{figure}[H]
    \centering
    % \includegraphics[width=0.7\textwidth]{figs/cf-secure-web-timeline-1.png}
    



\begin{tikzpicture}[
roundnode/.style={circle, draw=red!60, fill=red!5, very thick, minimum size=1mm},
squarednode/.style={rectangle, draw=red!60, fill=red!5, very thick, minimum size=5mm},
orangeroundnode/.style={circle, draw=orange!60, fill=orange!5, very thick, minimum size=1mm},
orangesquarednode/.style={rectangle, draw=orange!60, fill=orange!5, very thick, minimum size=5mm},
]
%Nodes
\node[roundnode]    at (0,16)      (start_http09)              [] {};
\node[squarednode]  at (14,16)     (end_http09)                [] {HTTP/0.9};

\node[roundnode]    at (1,15)      (start_http10)              [] {};
\node[squarednode]  at (14,15)     (end_http10)                [] {HTTP/1.0};

\node[roundnode]    at (2,14)      (start_http11)              [] {};
\node[squarednode]  at (14,14)     (end_http11)                [] {HTTP/1.1};

\node[roundnode]    at (3,13)      (start_spdyv1)              [] {};
\node[squarednode]  at (14,13)     (end_spdyv1)                [] {SPDY v1};

\node[roundnode]    at (4,12)      (start_spdyv2)              [] {};
\node[squarednode]  at (14,12)     (end_spdyv2)                [] {SPDY v2};

\node[roundnode]    at (5,11)      (start_quicrypto)              [] {};
\node[squarednode]  at (14,11)     (end_quicrypto)                [] {QUIC crypto};

\node[roundnode]    at (6,10)      (start_gquic)              [] {};
\node[squarednode]  at (14,10)     (end_gquic)                [] {gQUIC};

\node[roundnode]    at (7,9)      (start_ietfquic)              [] {};
\node[squarednode]  at (14,9)     (end_ietfquic)                [] {QUIC IETF};

\node[roundnode]    at (8,8)      (start_http3)              [] {};
\node[squarednode]  at (14,8)     (end_http3)                [] {HTTP/3};

\node[roundnode]    at (9,7)      (start_spdyv3)              [] {};
\node[squarednode]  at (14,7)     (end_spdyv3)                [] {SPDY v3};

\node[roundnode]    at (10,6)     (start_spdyv31)              [] {};
\node[squarednode]  at (14,6)     (end_spdyv31)                [] {SPDY v3.1};

\node[orangeroundnode]    at (0,5)      (start_ssl20)              [] {};
\node[orangesquarednode]  at (14,5)     (end_ssl20)                [] {SSL 2.0 (unsecure \cite{tls-external-facing-services})};

\node[orangeroundnode]    at (1,4)      (start_ssl30)              [] {};
\node[orangesquarednode]  at (14,4)     (end_ssl30)                [] {SSL 3.0 (unsecure \cite{tls-external-facing-services})};

\node[orangeroundnode]    at (2,3)      (start_tls11)              [] {};
\node[orangesquarednode]  at (14,3)     (end_tls11)                [] {TLS 1.1};

\node[orangeroundnode]    at (3,2)      (start_tls12)              [] {};
\node[orangesquarednode]  at (14,2)     (end_tls12)                [] {TLS 1.2};

\node[orangeroundnode]    at (4,1)      (start_tls13)              [] {};
\node[orangesquarednode]  at (14,1)     (end_tls13)                [] {TLS 1.3};



% \node[squarednode]      (http09_start)                              {HTTP/0.9};
% \node[squarednode]      (http09_end)        [right=of http09_start] {HTTP/0.9};
% \node[roundnode]        (lowercircle)       [below=of http09_start] {4};

% %Lines
\draw[->] (start_http09.east) -- (end_http09.west);
\draw[->] (start_http10.east) -- (end_http10.west);
\draw[->] (start_http11.east) -- (end_http11.west);
\draw[->] (start_spdyv1.east) -- (end_spdyv1.west);
\draw[->] (start_spdyv2.east) -- (end_spdyv2.west);
\draw[->] (start_quicrypto.east) -- (end_quicrypto.west);
\draw[->] (start_gquic.east) -- (end_gquic.west);
\draw[->] (start_ietfquic.east) -- (end_ietfquic.west);
\draw[->] (start_http3.east) -- (end_http3.west);
\draw[->] (start_spdyv3.east) -- (end_spdyv3.west);
\draw[->] (start_spdyv31.east) -- (end_spdyv31.west);

\draw[->] (start_ssl20.east) -- (end_ssl20.west);
\draw[->] (start_ssl30.east) -- (end_ssl30.west);
\draw[->] (start_tls11.east) -- (end_tls11.west);
\draw[->] (start_tls12.east) -- (end_tls12.west);
\draw[->] (start_tls13.east) -- (end_tls13.west);


\draw[->] (start_http09.east) .. controls +(right:5mm) and +(up:5mm) .. (start_http10.north);
\draw[->] (start_http10.east) .. controls +(right:5mm) and +(up:5mm) .. (start_http11.north);
\draw[->] (start_http11.east) .. controls +(right:5mm) and +(up:5mm) .. (start_spdyv1.north);
\draw[->] (start_spdyv1.east) .. controls +(right:5mm) and +(up:5mm) .. (start_spdyv2.north);
\draw[->] (start_spdyv2.east) .. controls +(right:5mm) and +(up:5mm) .. (start_quicrypto.north);
\draw[->] (start_quicrypto.east) .. controls +(right:5mm) and +(up:5mm) .. (start_gquic.north);
\draw[->] (start_gquic.east) .. controls +(right:5mm) and +(up:5mm) .. (start_ietfquic.north);
\draw[->] (start_ietfquic.east) .. controls +(right:5mm) and +(up:5mm) .. (start_http3.north);
\draw[->] (start_http3.east) .. controls +(right:5mm) and +(up:5mm) .. (start_spdyv3.north);
\draw[->] (start_spdyv3.east) .. controls +(right:5mm) and +(up:5mm) .. (start_spdyv31.north);


\draw[->] (start_ssl20.east) .. controls +(right:5mm) and +(up:5mm) .. (start_ssl30.north);
\draw[->] (start_ssl30.east) .. controls +(right:5mm) and +(up:5mm) .. (start_tls11.north);
\draw[->] (start_tls11.east) .. controls +(right:5mm) and +(up:5mm) .. (start_tls12.north);
\draw[->] (start_tls12.east) .. controls +(right:5mm) and +(up:5mm) .. (start_tls13.north);

    \draw [-stealth](0,0)  -- (15,0) node[midway, above] {Time};
    
\end{tikzpicture}

\caption[Evolution timeline of the related network technologies]{Evolution timeline of related network technologies. This diagram is taken from \texttt{Cloudflare}'s blog~\cite{http-3-from-root-to-tip}.\simon{UNIMPORTANT TODO: add dates on the timeline axis}}
    \label{fig:cf-secure-web-timeline}
    \end{figure}


\section{\texttt{QUIC} Details} \label{QUIC_details_section}

\texttt{QUIC} is a next-generation transport layer protocol whose network stack is demonstrated in Figure~\ref{fig:QUIC_network_stack}. In this diagram, the \texttt{UDP} transport layer protocol is smaller than the \texttt{TCP} layer because \texttt{UDP} only provides multiplexing between the two processes of the two end hosts by using port numbers~\cite[Chapter 5.1]{PetersonLarryL2007CNAS}.
\texttt{UDP} sends packets in \enquote{fire and forget} manner~\cite{Google_QUIC_protocol_moving_the_web_from_TCP_to_UDP}.
Hence, \texttt{UDP} does not guarantee reliable packet delivery, and it does not provide congestion control~\cite[Chapter 5.1]{PetersonLarryL2007CNAS}.
However, \texttt{UDP} checksum adds a mechanism to verify that \texttt{UDP} packets are valid and arrive at the intended recipients
\cite[Chapter 5.1]{PetersonLarryL2007CNAS}, \cite{how-is-tcp-and-udp-checksum-calculated, udp-user-datagram-protocol}.

% [TODO: WHAT end-to-end LATENCY?]
\texttt{TLS1.3} layer in Figure~\ref{fig:QUIC_network_stack} is shown as being part of the \texttt{QUIC} protocol because, as I have already mentioned in \S~\ref{Networking_Landscape_Before_QUIC},  \texttt{QUIC} aims to reduce end-to-end latency by combining handshakes of both transport (\texttt{QUIC}) and security (\texttt{TLS1.3}) layers~\cite{Google_QUIC_protocol_moving_the_web_from_TCP_to_UDP, HTTP_3_the_past_the_present_and_the_future}.

Finally, \texttt{QUIC} requires a completely new application layer protocol, called \texttt{HTTP/3}, that is only compatible with \texttt{QUIC}~\cite{head-of-line-blocking-in-quic-and-http-3-the-details}.
For completeness I will mention that \texttt{HTTP/3} layer is smaller than analogous \texttt{HTTP/2} layer because some tasks usually associated with the application layer (e.g., stream multiplexing and connection management) are delegated to \texttt{QUIC}~\cite{bib_grigorik2013}, \cite[Chapter 12]{Google_QUIC_protocol_moving_the_web_from_TCP_to_UDP}.


% TODO - combine these ideas to explain the smaller HTTP3
% because \enquote{(...) data across streams is no longer fully  ordered}~\cite{head-of-line-blocking-in-quic-and-http-3-the-details}.
% TODO: what does it mean for QUIC streams not to be ordered

    \begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\textwidth]{figs/Layers_in_the_protocol_stack.png}
    \caption[\texttt{QUIC/UDP/IP} network stack compared to \texttt{TCP/IP} network stack]{\texttt{QUIC/UDP/IP} network stack compared to \texttt{TCP/IP} network stack. This diagram combines ideas of Robin Marx's presentation about \texttt{QUIC}~\cite{head-of-line-blocking-in-quic-and-http-3-the-details} as well as other sources \cite{overview_of_the_QUIC_protocol, IETF_presentation_about_QUIC, HTTP_3_the_past_the_present_and_the_future}.
    }
    \label{fig:QUIC_network_stack}
    \end{figure}



% ------------------------------
% TODO: maybe incorporate the paragraph below:
%
% As Mattias Geniar wrote in Chapter 12~\cite{Google_QUIC_protocol_moving_the_web_from_TCP_to_UDP}, \texttt{HTTP over QUIC} layer (which is now called \texttt{HTTP/3}~\cite{HTTP_3_the_past_the_present_and_the_future}) is smaller than analogous \texttt{HTTP/2} layer because stream multiplexing and connection management tasks are delegated to \texttt{QUIC}~\cite{bib_grigorik2013}.
% ------------------------------


As demonstrated in Figure~\ref{fig:IP_UDP_QUIC_packets}, \texttt{QUIC} packets are encapsulated inside \texttt{UDP} packets which in turn are sent using \texttt{IP} packets.


    \begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{figs/IP_UDP_QUIC packets.png}
    \caption[The relative packet structure of \texttt{IP/UDP/QUIC} network stack]{The relative packet structure of \texttt{IP/UDP/QUIC} network stack. This diagram is drawn from the latest \texttt{IETF}'s description of \texttt{QUIC}~\cite{ietf-quic-transport-34}}
    \label{fig:IP_UDP_QUIC_packets}
    \end{figure}
    
\simon{Figure~\ref{fig:IP_UDP_QUIC_packets} could be removed if there is a lack of space}



An example of \enquote{real-life} \texttt{QUIC} packets, captured using \texttt{Wireshark} network analysis tool, is shown in Figure~\ref{fig:Wireshark_screenshot}.



    \begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{figs/Short_QUIC_packet_header_format.PNG}
    \caption[Short \texttt{QUIC} packet header format]{Short \texttt{QUIC} packet header format. The diagram is taken from \cite{UnderstandQUIC}. \simon{MAYBE TODO: describe QUIC packets and their field values (see Figure~\ref{fig:QUIC_packet_format})}}
    \label{fig:QUIC_packet_format}
    \end{figure}



\subsection{\texttt{QUIC} Features} \label{QUIC_features}
% https://archive.nanog.org/sites/default/files/meetings/NANOG64/1051/20150603_Rogan_Quic_Next_Generation_v1.pdf
% TODO: read https://tools.ietf.org/html/draft-ietf-quic-transport-32#section-1
% TODO: read https://arxiv.org/pdf/1801.05168.pdf
\begin{itemize}


 \item \textbf{Connection migration} 
 % (https://peering.google.com/\#/learn-more/quic, https://ma.ttias.be/googles-quic-protocol-moving-web-tcp-udp/)
 
    % As I have already mentioned in the Introduction section, more than 90\% of all Internet users use mobile devices to connect to the Internet~\cite{bib_number_of_mobile_users}.
    Nowadays, ubiquitous mobile devices~\cite{bib_number_of_mobile_users} can change their location, and thus their Internet access can frequently change too~\cite{PollardBarry2019HiAP}.
    This is particularly problematic for \texttt{TCP} because \texttt{TCP's} connection is tied to a particular \texttt{IP} address~\cite{PollardBarry2019HiAP}.
    As a result, \texttt{TCP} needs to establish a new connection every time the Wi-Fi network is changed~\cite{PollardBarry2019HiAP}.
    This means that frequent access point changes would hurt \texttt{TCP's} performance because \texttt{TCP} would have to spend more time in the \enquote{slow-start} phase and perform additional handshakes.
    In addition, because of the different signal paths, other access points may have distinct round-trip times.
    Hence, after changing its access point, the mobile device may incorrectly use RTT estimates from the previous Wi-Fi connection for the congestion control over the new access point.
    This example demonstrates that \texttt{TCP} protocol assumes that \texttt{IP} address would not frequently change~\cite{PollardBarry2019HiAP}.
    In contrast, \texttt{QUIC} is more suitable to handle connections via mobile devices.
    In particular, \texttt{QUIC} has an explicit connection ID that does not depend on the IP address~\cite{PollardBarry2019HiAP}.
    Hence, \texttt{QUIC} clients can migrate between multiple different \texttt{IP} networks without having to re-establish a \texttt{QUIC} connection.
    
    
    





\item \textbf{Fast \texttt{TLS1.3} handshake}

As shown in Figure~\ref{fig:QUIC_handshake_vs_TCP-TLS_handshake}, one of the core innovations of \texttt{QUIC} is to combine the transport and security layer handshakes, thus reducing latency of the first \texttt{HTTP} request.


     \begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{figs/QUIC_handshake_vs_TCP-TLS_handshake.png}
    \caption[Comparison of handshakes required to issue an \texttt{HTTP} request]{Comparison of handshakes required to issue an \texttt{HTTP} request. The left section demonstrates that traditionally multiple round trip times are required to establish a connection to issue an \texttt{HTTP} request. The right section of the diagram shows that \texttt{QUIC}, on the other hand, combines transport and security layer handshakes. This diagram is a reproduction of a similar diagram taken from~\cite{the-road-to-quic}.}
    \label{fig:QUIC_handshake_vs_TCP-TLS_handshake}
    \end{figure}
 
 % TODO: Why do we need a TCP handshake?
 % TODO: why do we need a TLS handshake?
 % TODO: what info is sent in \texttt{QUIC} handshake?
 % MAYBE TODO: mention possible amplification attacks of \texttt{QUIC} (https://blog.cloudflare.com/the-road-to-quic/) 
 

  \item 
    According to the \texttt{IETF QUIC} draft, Section 1~\cite{ietf-quic-transport-draft-32}, \texttt{QUIC} offers \textbf{zero round-trip-time connections} (abbreviated \texttt{0-RTT}) provided that the server and client had an established connection in the recent past~\cite{introducing-0-rtt}.
    In other words, under the aforementioned circumstances, \texttt{QUIC} server and client can start exchanging data without having to wait for the completion of the new handshake~\cite{ietf-quic-transport-draft-32}.
  
  % \simon{If needed, add more details from: \url{https://qlog.edm.uhasselt.be/epiq/files/QUICImplementationDiversity_Marx_final_11jun2020.pd}}
  

  \item 
  An interesting design choice of \texttt{QUIC} is that it is \textbf{implemented in user space rather than kernel space}~\cite{the-road-to-quic, Implementation_and_Performance_Evaluation_of_the_QUIC_Protocol_in_Linux_Kernel, quic-and-http-3-too-big-to-fail}.
  The major advantage of this approach is that new versions of \texttt{QUIC}, associated with user-level programs, can be quickly developed and deployed (e.g., by deploying protocol updates with the software updates)~\cite{the-road-to-quic}.
  This explains why there are numerous different implementation of \texttt{QUIC} (see \S~\ref{List_of_QUIC_implementations}).
  In addition, all the major Operating Systems, e.g., Microsoft Windows~\cite{OS_implementations}, Linux~\cite{OS_implementations, linux_kernel_programming_language}, iOS~\cite{OS_implementations, IOKitFundamentals}, are written in the C language family.
  There are numerous bootstrapping compilers written in C, meaning that any user-level program, potentially written in a different programming language, can be compiled and run.
  In addition, despite several exceptions such as Google, Microsoft, and Apple, operating systems may not be controlled by the same companies as the ones that develop client-side software, such as browsers~\cite{2016-12-01-quic-tou}.
  Hence, there are additional incentives for the companies developing \texttt{QUIC} to deploy this protocol in userspace.
  
  However, there is an associated cost of the user space because data has to be copied from kernel space to user space~\cite{benchmarking-quic}.
  Furthermore, Langley et al. found that \texttt{YouTube's} servers used 350\% more \texttt{CPU} resources when delivering content via \texttt{QUIC} connections as opposed to using \texttt{TCP} with \texttt{TLS}~\cite{The_QUIC_Transport_Protocol_Design_and_Internet-Scale_Deployment}.
  In addition, by deploying \texttt{QUIC} changes in the user space, companies can also patch proprietary extensions.
  Firms can even use their market dominance (e.g., in the browser industry) to enforce these protocol updates.
  However, there is no guarantee that proprietary extensions would improve network quality for the general public. 




 
  \item \texttt{QUIC} features include packet \enquote{\textbf{encryption by default}}~\cite{the-road-to-quic}.
  Moreover, there are no implementations of \texttt{QUIC} without embedded security layer~\cite{head-of-line-blocking-in-quic-and-http-3-the-details}
  (just a side note, \texttt{MsQuic}, one implementation of \texttt{QUIC}, does provide a mechanism to turn off packet encryption after finishing a cryptographically secure handshake).
  However, there is some criticism that Internet Service Providers could be having problems in the future managing encrypted, and thus non-transparent, \texttt{QUIC} traffic inside their networks~\cite{why-is-googles-quic-leaving-network-operators-in-the-dark}.
  The concern is that conventional firewalls would not be able to reject suspicious \texttt{QUIC} packets~\cite{quic-the-internet-transport-protocol-based-on-udp}.
  In addition, network engineers may find it more difficult to analyse encrypted \texttt{QUIC} headers to understand traffic patterns~\cite{2017-12-18-transport-header-encryption}.
  
  Each \texttt{QUIC} packet is encrypted and decrypted individually in order to avoid the \texttt{TLS} head-of-line blocking~\cite{head-of-line-blocking-in-quic-and-http-3-the-details}.
  In contrast, an alternative approach would be to group packets for decryption, hoping to amortise the cost of cryptographic function calls~\cite{head-of-line-blocking-in-quic-and-http-3-the-details, optimizing-tls-record-size-and-buffering-latency}.
  However, one missing packet from the group would prevent other packets from being decrypted (and thus transferred to the application layer)~\cite{head-of-line-blocking-in-quic-and-http-3-the-details}.


  
  \end{itemize}
  Some background knowledge about \texttt{QUIC's} streams is required to understand subsequent \texttt{QUIC} features.
  \texttt{QUIC} uses streams to overcome the aforementioned Head-of-line blocking problem~\cite{UnderstandQUIC}.
  The logical implementation of streams is similar to the one used for \texttt{HTTP/2.0} (see Figure~\ref{fig:Multiplexing_HTTP2}).
  However, instead of using a shared \texttt{TCP} connection, \texttt{QUIC} uses a logical \texttt{QUIC} connection.
  Every \texttt{QUIC} stream is separated into frames~\cite{UnderstandQUIC},~\cite[Chapter~2.1]{ ietf-quic-transport-draft-32} and there may be several \texttt{QUIC} frames in a single \texttt{QUIC} packet ~\cite[Chapter~1]{ ietf-quic-transport-draft-32}.
  Furthermore, each frame stores information about the stream it belongs to as well as the range of the application layer data bytes it transfers~\cite{UnderstandQUIC}.
  \begin{itemize}
  
  
  \item \texttt{QUIC} provides \textbf{flow control} that is implemented using streams.
  \texttt{QUIC} provides both per-stream and per-connection flow control fairness \cite{ietf-quic-transport-draft-32}.
  In other words, there are limits associated with each connection that specify the number of bytes that the sender is allowed to transfer via a particular stream.
  The purpose of per-stream limits is to prevent streams from consuming all the available bandwidth available to the connection.
  In addition, there is the maximum number of bytes that sender can transfer to the receiver over the given connection at any single point in time~\cite{ietf-quic-transport-draft-32}.
  This is a conventional mechanism also seen in \texttt{TCP} that prevents the slower receiver from being flooded with the sender's messages~\cite{ietf-quic-transport-draft-32}.
  
  
    \item \texttt{QUIC} also provides \textbf{reliable in-order data delivery}~\cite[Section 7]{ietf-quic-transport-draft-32}, ~\cite{head-of-line-blocking-in-quic-and-http-3-the-details} .
    The primary purpose of this is to replace \texttt{TCP} by offering almost identical guarantees. 
    However, there is an important point to mention that actually, the delivery order is preserved within a single \texttt{QUIC} stream~\cite{head-of-line-blocking-in-quic-and-http-3-the-details},~\cite[Section 2]{ietf-quic-transport-draft-32}.
    Otherwise, because of the multiplexed streams, data transferred via different \texttt{QUIC} streams could be re-ordered on the receiver's side between different streams~\cite{head-of-line-blocking-in-quic-and-http-3-the-details},~\cite[Section 2]{ietf-quic-transport-draft-32}.
    On the one hand, the change in the total receive order is not important if separate files are transferred via different \texttt{QUIC} streams~\cite{head-of-line-blocking-in-quic-and-http-3-the-details}, which is usually the case with modern websites that send multiple embedded objects.
    But on the other hand, unguaranteed packet ordering between \texttt{QUIC} streams changed \texttt{HTTP} semantics.
    As a result, the new custom application layer, called \texttt{HTTP/3}, had to be built for \texttt{QUIC}~\cite{head-of-line-blocking-in-quic-and-http-3-the-details}.
    In other words, to avoid the transport-layer head of line problem, \texttt{QUIC} was made aware of the different streams~\cite{head-of-line-blocking-in-quic-and-http-3-the-details}.
    Hence, for this reason, \texttt{QUIC} in Figure~\ref{fig:QUIC_network_stack} is shown as also being part of the application layer~\cite{head-of-line-blocking-in-quic-and-http-3-the-details}.
    
    
    
    % ----------------------
    
    It is important to mention that \texttt{QUIC} uses unique monotonically increasing packet sequence numbers~\cite{UnderstandQUIC, The_QUIC_Transport_Protocol_Design_and_Internet-Scale_Deployment},~\cite[Section~4.2]{ietf-quic-recovery-32}.
    As a result, lost \texttt{QUIC} frames are retransmitted over \texttt{QUIC} packets with different sequence numbers~\cite[Section~4]{ietf-quic-recovery-32},~\cite{UnderstandQUIC}.
    In contrast, \texttt{TCP} reuses packet numbers for retransmissions~\cite[Section~4]{ietf-quic-recovery-32}.
    Hence, \texttt{QUIC's} approach is superior because it can determine the exact number of packets in transit more accurately~\cite{UnderstandQUIC}.
    This concept is visualised in Figure~\ref{fig:Impact_of_increasing_pkt_numbers}.
    This diagram demonstrates that \texttt{QUIC} can cope with repeated packet loss better than \texttt{TCP}~\cite{UnderstandQUIC}.
    Furthermore, \texttt{QUIC}, as opposed to \texttt{TCP}, can identify whether or not a received packet contains retransmitted frames~\cite{UnderstandQUIC}.
    This, in turn, allows \texttt{QUIC} to estimate round-trip time more accurately.
    Figure~\ref{fig:Impact_of_increasing_pkt_numbers} also demonstrates that \texttt{QUIC} uses frame byte ranges to identify missing frames and provide reliable delivery~\cite{UnderstandQUIC}.
    In subsequent paragraphs, I describe how \texttt{QUIC} packet numbers are used to avoid network congestion~\cite{UnderstandQUIC}.
    
    \begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{figs/Impact_of_increasing_pkt_numbers.png}
    \caption[Increasing packet numbers used by \texttt{QUIC} allow the protocol to differentiate between the packet loss and reordering]{Increasing packet numbers used by \texttt{QUIC} allow the protocol to differentiate between the packet loss and reordering. The diagram visualises Hunter Dellaverson et al. ideas~\cite{UnderstandQUIC}.}
    \label{fig:Impact_of_increasing_pkt_numbers}
    \end{figure}
    
    
    
    \item  \texttt{QUIC} has \textbf{loss recovery} mechanism.
    \texttt{QUIC} uses two ways to detect packet loss - by tracking unacknowledged packet numbers and using timeouts~\cite{UnderstandQUIC},~\cite[Section~6]{ietf-quic-recovery-32}.
    An important point to mention is that a \texttt{QUIC} packet may be declared as lost if it gets reordered, and some number (e.g., three for \texttt{ngtcp2}) of subsequent packets are acknowledged before the reordered packet,~\cite[Section~6]{ietf-quic-recovery-32}.
    In case of the packet loss, \texttt{QUIC} passes other successfully received packets to the application layer above provided that a missing packet is from another stream~\cite{head-of-line-blocking-in-quic-and-http-3-the-details}.
   
   \item \texttt{QUIC} implements \textbf{congestion control} because, as I have already mentioned in the previous sections, the underlying transport protocol layer \texttt{UDP} does not provide this mechanism. 
   The same \texttt{QUIC} congestion control algorithm is applied over all \texttt{QUIC} streams~\cite{head-of-line-blocking-in-quic-and-http-3-the-details, UnderstandQUIC}.
   Similarly, round-trip time estimations are also shared among different streams~\cite[Section~4.2]{ietf-quic-recovery-32}.
   As a result, a single lost packet on one stream would likely have an impact on the throughput of all streams, but a lost packet of one stream would not block the other streams~\cite{head-of-line-blocking-in-quic-and-http-3-the-details, UnderstandQUIC}.
   Actually, \texttt{QUIC} has to observe multiple systematic packet losses for a noticeable period of time in order to declare that there is persistent congestion in the network ~\cite[Section~4]{ietf-quic-recovery-32},~\cite{UnderstandQUIC}.
   Then, the sender can reduce its congestion window~\cite{UnderstandQUIC}.
   As a side note, \texttt{ngtcp2}, the primary \texttt{QUIC} implementation that I am studying, uses \texttt{CUBIC} as the default congestion control mechanism (according to the private correspondence with the \texttt{ngtcp2} co-author).
  
  

 

  
  
  \item The effect of the \textbf{reduced latency} of \texttt{QUIC} is apparent.
  For example, engineers in Google discovered that, when compared with \texttt{TCP}, \texttt{QUIC} substantially improved user experience for the customers that used the low-quality Internet links, e.g. wireless links~\cite{chromium_blog_about_quic, quic-vs-tcptls-and-why-quic-is-not-the-next-big-thing}.
  In particular, the study revealed that in some cases, the time required to download contents of Google's main website was reduced by one second ~\cite{chromium_blog_about_quic}.
  One possible explanation of this effect could be that any packet loss decreases the effective \texttt{TCP} throughput (See Equation~\ref{eq:gass_stuff}).
  
  Steady state throughput formula, as defined by Jitendra Padhye et.al.~\cite{TCP_throughput_formula}: 
    \begin{equation}
    \textrm{TCP throughput} (p) \approx \frac{1}{\textrm{RTT}}\sqrt{\frac{1}{2bp}} 
    \label{eq:gass_stuff}
    \end{equation}
     where $p$ is the packet loss rate, $b$ is the number of packets acknowledged with a single acknowledgement message, and RTT being the round-trip time.
  
  
  However, under a substantial but tolerable packet loss rate, \texttt{QUIC} can avoid decreasing its congestion window if it does not detect persistent congestion.
  
 %\simon{If I remember correctly, (as someone said) the problem with TCP is that it confuses network congestion with unreliable links}
  
\end{itemize}



\subsection{Details about Encryption of \texttt{QUIC}} \label{subsection_QUIC_header_format}


\texttt{QUIC} partially encrypts packet headers and fully encrypts packet payloads~\cite{the-road-to-quic}.
However, packet header encryption is separate from the payload encryption
\simon{TODO: FIND SOURCE}.
Header encryption hides some privacy information that could be otherwise used to match traffic patterns of a single user~\cite{2017-12-18-transport-header-encryption, the-road-to-quic}.
According to the \texttt{QUIC} IETF draft, obfuscated packet numbers provide only partial confidentiality~\cite{ietf-quic-transport-draft-32}, as connection IDs are not encrypted~\cite[Section~5.4.1]{ietf-quic-tls-32}.
However, the primary purpose of hidden packet numbers is to prevent middleboxes from interfering and thus ossifying the protocol~\cite{the-road-to-quic}.
In contrast, packet payloads are authenticated and encrypted to secure and hide application layer data~\cite{ietf-quic-tls-32}.

  
 % \simon{TODO present long and short headers give a list of fields (maybe to Appendix)}
  
\subsection{\texttt{QUIC}'s Adoption}

At the time of writing, $5.0\%$ of all the websites on the Internet used \texttt{QUIC} and $14.5\%$ used \texttt{HTTP/3}~\cite{bib_Adoption_comparison_Between_http2_http3_quic}.
Similarly, according to Cloudflare\footnote{https://radar.cloudflare.com/}, at the same time 11\% of traffic used \texttt{HTTP/3}.



\section{Implementations of QUIC} \label{List_of_QUIC_implementations}
According to \texttt{QUIC} Working Group~\cite{number_of_QUIC_implementations}, there is 31 different implementation of QUIC. Most of the implementations, 22 to be precise, are written using C/C\texttt{++} language family (e.g., \texttt{Chromium}\footnote{https://chromium.googlesource.com/chromium/src/net/+/master/quic/}) while the remaining implementations use Rust (e.g., \texttt{quiche}\footnote{https://github.com/cloudflare/quiche}), Python (e.g., \texttt{aioquic}\footnote{https://github.com/aiortc/aioquic}), Haskel (e.g., \texttt{Haskell quic}\footnote{https://github.com/kazu-yamamoto/quic}), Java (e.g., \texttt{kwik}\footnote{https://bitbucket.org/pjtr/kwik/src/master/}), Go (e.g., \texttt{quic-go}\footnote{https://github.com/lucas-clemente/quic-go}) or JavaScript (e.g., \texttt{Node.js QUIC}\footnote{https://github.com/nodejs/quic}).




\subsection{Selecting  \texttt{ngtcp2}}
%TODO: fix style and grammar:
There are many implementations of \texttt{QUIC} (see subsection \ref{List_of_QUIC_implementations}) written in different languages.
I considered  \texttt{aioquic}\footnote{https://github.com/aiortc/aioquic},  \texttt{MsQuic}\footnote{https://github.com/microsoft/msquic},  \texttt{ngtcp2}\footnote{https://github.com/ngtcp2/ngtcp2},  \texttt{mvfst}\footnote{https://github.com/facebookincubator/mvfst} and \texttt{Quant}\footnote{https://github.com/NTAP/quant}.
Before changing the direction of the dissertation, one of the primary goals of the project was to turn off header encryption of \texttt{QUIC} packets so that I could inspect these packets in hardware.
Hence, I performed a survey to find \texttt{QUIC} implementation, which had an API to control encryption. 
\texttt{Aioquic}, \texttt{MsQuic} and \texttt{ngtcp2} seemed to be the most promising.
However, despite being a simple and well-documented implementation of \texttt{QUIC}, \texttt{aioquic} did not have an active development team that could answer technical questions.
Similarly, \texttt{MsQuic} seemed promising, but some of the performance measurement tools used by the \texttt{MsQuic} team were proprietary and not yet released to the public. 
I thought that in this dissertation, it would be wise to use similar testing conditions as the ones used in the previous similar paper (Yang et al.~\cite{Making_QUIC_Quicker}).
In particular, the authors of this paper evaluated several  or C\texttt{++} \texttt{QUIC} implementations.
Hence, I decided to follow a similar approach, and I picked \texttt{ngtcp2}, written in C.
I identified the experimental \texttt{QUIC} benchmarking tool called \texttt{h2load}\footnote{https://github.com/nghttp2/nghttp2/tree/quic}, which is compatible with \texttt{ngtcp2}.
Moreover, \texttt{h2load} is written by Mr Tatsuhiro  Tsujikawa (a co-author of \texttt{ngtcp2}).
Both \texttt{h2load} and \texttt{ngtcp2} use similar library functions meaning that I could deploy software updates to both programs with minimal effort.





\section{Benchmarking tools}
% Frame pointer points to the top/bottom of the stack (from https://www.youtube.com/watch?v=nXaxk27zwlk&ab_channel=CppCon)


\subsection{iperf3 and nuttcp}
\texttt{Iperf3}\footnote{\url{https://github.com/esnet/iperf}} is a tool that generates artificial \texttt{TCP} or \texttt{UDP} traffic and thus measures throughput of these protocols.
\texttt{Iperf3} performs throughput measurements between \texttt{iperf3} server and client.
One can assume that \texttt{UDP}'s throughput is a theoretical maximum of \texttt{QUIC} throughput as \texttt{QUIC} builds on top of \texttt{UDP}.
However, as noted in~\ref{QUIC_details_section}, \texttt{UDP} does not guarantee reliable packet delivery, so the sender and receiver could see different throughput values.
Hence, to put \texttt{QUIC} into perspective, it is worth comparing its throughput against the throughput of the similar \texttt{TCP} protocol.
An important aspect of \texttt{TCP} is that its congestion control mechanism allows the sender to discover an appropriate sending bitrate.
In contrast, \texttt{UDP} does not take into account the state of the network.
Consequently, if unlimited \texttt{UDP} throughput is allowed, then one could flood the network by running \texttt{iperf3} to measure \texttt{UDP} throughput.
However, as shown in Figure~\ref{fig:Wireshark_separation}, performance analysis tests were executed on the separate testing network, so no damage was done to the public network.

There were other similar throughput measurement tools (e.g., \texttt{nuttcp}\footnote{http://nuttcp.net/nuttcp/} or \texttt{iperf2}\footnote{https://sourceforge.net/projects/iperf2/}).
In particular, \texttt{nuttcp} is said to be better for high \texttt{UDP} throughput measurements~\cite{network-troubleshooting-tools_nuttcp} and it ought to provide more stable results when analysing \texttt{UDP}~\cite{Nuttcp_geant}.
However, my primary throughput measurement tool was \texttt{iperf3} because it simplified experiments of variable-size files.
In addition, I extensively used \texttt{iperf3's} JSON ouput format.
Despite that, I sometimes used \texttt{nuttcp} when measuring throughput for fixed periods of time. 

% TODO
% \simon{TODO: mention that one can adjust UDP send or receive (TODO check which one) buffer size}


\subsection{ping}
\texttt{Ping} is a network analysis tool used to measure the round trip time between the two communicating parties.
\texttt{Ping} uses Internet Control Message Protocol (or \texttt{ICMP} for short) to issue a request-reply pair~\cite{internet-control-message-protocol-icmp}.
Round trip time measurements were used to test the correctness of the testing environment (e.g. to measure additional network delay induced by the intermediate machine).
However, on \texttt{ping} could not be used to measure one-way delay because network delays can be asymmetric.





\subsection{Wireshark}

\texttt{Wireshark}\footnote{https://www.wireshark.org/} is a conventional tool used to inspect captured network packets (see Figure~\ref{fig:Wireshark_screenshot}).
\texttt{Wireshark} visualises key information about the packets -- source and destination \texttt{IP} addresses, type of protocol and time when the packet was received on the specified network interface.

    \begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{figs/Wireshark_screenshot.png}
    \caption[Example of \texttt{QUIC} packets captured with \texttt{Wireshark}]{Example of \texttt{QUIC} packets captured with \texttt{Wireshark}. This diagram demonstrates that \texttt{Wireshark} can decrypt, parse and display packet headers of \texttt{QUIC}. In particular, packet numbers (abbreviated PKN) are and displayed on the right hand side.}
    \label{fig:Wireshark_screenshot}
    \end{figure}
    
When required, I ran \texttt{Wireshark} on the intermediate machine to inspect the size of the packets transferred between the \texttt{QUIC} client and server.
It is worth mentioning that \texttt{QUIC} packets are encrypted.
Hence, I had to share the server's cryptographic certificate with \texttt{Wireshark} so that it could decrypt and extract relevant fields (e.g. packet number) of observed \texttt{QUIC} packets.

I used \texttt{Wireshark} in an ethical manner, meaning that I only used it to analyse traffic between the dedicated network interfaces.
In other words, as demonstrated in Figure~\ref{fig:Wireshark_separation}, I did not capture packets from the college network because the testing environment used separate network interfaces. 

    \begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{figs/Wireshark_separation.png}
    \caption[Separation of testing and college networks]{Separation of testing and college networks. I performed testing on a private network, and I did not use \texttt{Wireshark} to capture packets from the college network}
    \label{fig:Wireshark_separation}
    \end{figure}



\subsection{perf}
\texttt{Perf}\footnote{https://perf.wiki.kernel.org/index.php/Main\_Page} is a performance analysis tool that measures CPU performance by sampling the call stack.
However, sampling-based performance analysers add lower overhead than event-based analysers because the performance measurement code is not executed for almost every instruction.
Instead, \texttt{perf} inspects the CPU stack almost one thousand times a second to identify the most problematic functions.
In particular, I used a sampling frequency of 997 samples per second in order to avoid introducing bias of sampling \enquote{round numbers}~\cite{perf-cpu-sample}.
However, during this project, I realised that sampling-based performance analysers only work well if the program runs for a sufficiently long time and multiple stacks can be captured.



\subsection{Flame Graphs}
\texttt{Flame Graphs} \footnote{\url{http://www.brendangregg.com/flamegraphs.html}} is a performance analysis tool that is used to visualise the performance of the program.
In this project I used CPU \texttt{Flame Graphs} that show the aggregated call stacks of numerous samples (see Figure~\ref{fig:perf_results_of_h2load}).
To understand the \texttt{Flame Graphs}, it is worth looking at the simple example diagram first (see Figure~\ref{fig:Flame_Graphs_explanation}).

    \begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{figs/Flame_Graphs_explanation.png}
    \caption[Schematic diagram of \texttt{Flame Graphs}]{Schematic diagram of \texttt{Flame Graphs} adopted from Brendan Gregg's presentation~\cite{USENIX_ATC2017_flamegraphs}}
    \label{fig:Flame_Graphs_explanation}
    \end{figure}

Here y-axis of \texttt{Flame Graphs} represents the depth and frames of the call stack~\cite{CPU_Flame_graphs}.
For instance, as shown in Figure~\ref{fig:Flame_Graphs_explanation}, \textbf{function\_a()} invoked function calls \textbf{function\_b()} and \textbf{function\_g()}. 
However, an important point to mention is that x-axis of \texttt{Flame Graphs} does not represent time~\cite{CPU_Flame_graphs}.
Instead, function names are sorted alphabetically along the x-axis, and it represents the approximate percentage of stack frames that were in the call stack~\cite{CPU_Flame_graphs}.
Hence, by looking at Figure~\ref{fig:Flame_Graphs_explanation} it is possible to tell that \textbf{function\_b()} and its \enquote{children} functions spent more time on CPU than \textbf{function\_g()} and its corresponding functions but it is impossible to tell the relative order in which these functions were called.
Furthermore, in order to better identify the most frequent function calls, stack frames sampled at different times can be merged if they have a shared ancestor~\cite{CPU_Flame_graphs}.
As a result, by looking at Figure~\ref{fig:Flame_Graphs_explanation} one can not differentiate whether or not function calls \textbf{function\_b()} and \textbf{function\_g()} did not interleave.

In the standard \texttt{Flame Graph}, different frames are coloured differently in order to increase visual contrast between the neighbouring cells in the diagram~\cite{CPU_Flame_graphs} (see yellow, orange and red rows in Figure~\ref{fig:perf_results_of_h2load}).

    \begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{figs/perf_results_of_h2load.png}
    \caption[Typical \texttt{perf} results of running \texttt{h2load} client against \texttt{ngtcp2} server on the same machine (A), when packets are transferred via an intermediate machine (B)]{Typical \texttt{perf} results of running \texttt{h2load} client against \texttt{ngtcp2} server on the same machine (A), when packets are transferred via an intermediate machine (B). This graph shows that about 70\% of the time, default \texttt{ngtcp2} version spends doing cryptographic operations} 
    \label{fig:perf_results_of_h2load}
    \end{figure}


However, in this project, I am also using \texttt{Differential Flame Graphs} that depict the difference between the two \texttt{Flame Graphs}~\cite{Differential_Flame_Graphs}.
The structure of \texttt{Differential Flame Graph} is identical to the standard \texttt{Flame Graph}, but the colouring information identifies which functions were more or less common in the second \texttt{Flame Graph} when compared with the reference \texttt{Flame Graph}~\cite{Differential_Flame_Graphs} (see Figure~\ref{fig:perf-kernel__johns_BIG_letter__encrypted_minus_perf-kernel__johns_BIG_letter__unencrypted}).

    \begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{figs/perf-kernel__johns_BIG_letter__encrypted_minus_perf-kernel__johns_BIG_letter__unencrypted.png}
    \caption[Difference between aggregated CPU stacks of sending encrypted and unencrypted 100MB files]{Difference between aggregated CPU stacks of sending encrypted and unencrypted 100MB files. The red areas in the centre represent removed cryptographic functions from the unencrypted \texttt{ngtcp2} version which that previously formed a CPU bottleneck for \texttt{QUIC}}
    \label{fig:perf-kernel__johns_BIG_letter__encrypted_minus_perf-kernel__johns_BIG_letter__unencrypted}
    \end{figure}




% \simon{[MAYBE TODO: mention that I had to enable additional compiler flags to be able to see deeper call frames -- before that they were optimised by compiler and some stacks were partially anonymous  -- add a reason why this problem occurred -- compiler optimised one register?]}


    
\subsection{\texttt{Qlog} and \texttt{qvis}}

\texttt{Qlog} is a new standard format to record \texttt{QUIC} events~\cite{draft-marx-qlog-event-definitions-quic-h3}.
Some \texttt{QUIC} implementations, such as \texttt{ngtcp2}, can produce these event logs from both server's and client's perspective.
Then, I used \texttt{qvis}\footnote{\url{https://qvis.quictools.info/##/files}}, \texttt{QUIC} event visualisation tool, to debug \texttt{QUIC} (see Figure~\ref{fig:qvis}).


    \begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{figs/qvis.png}
    \caption[Example of \texttt{qvis} visualisation]{Example of \texttt{qvis} visualisation. This diagram demonstrates that \texttt{server} is unable to send a reply to the \texttt{client}.}
    \label{fig:qvis}
    \end{figure}
    



\section{Requirements Analysis}

In order to measure the performance of \texttt{QUIC}, I had to select suitable network simulation tools.
I could have used localhost for performance measurements.
On the one hand, it is relatively simple to set up such experiments.
Hence, I used localhost connections for quick prototyping and unit tests.
However, the major disadvantage of using localhost is that it is artificial, so the performance measurements may not reflect actual networking conditions.
For instance, packets sent via localhost are memory-copied from one place on the machine to another, meaning that \texttt{QUIC} packets would not actually leave the machine.
Hence, the tests could not measure the impact of Network Interface Cards (NICs) (e.g., offloading functionality).
Another important feature of localhost connections is that they use parameters that are not usually seen on the global Internet.
For example, usually, localhost connections use a maximum transmission unit (MTU) of around 65000 bytes, while in the actual Internet, it is usually around 1500 bytes.

 Alternatively, I could have forced packets to leave the testing machine but send them via loopback cable to the same machine on a different interface. 
 That would have been still a relatively simple set-up. 
 Moreover, I would have allowed me to inspect the behaviour of NICs.
 Traffic would have to be added on the same testing machine.
 Hence, Network Emulation could have interfered with the analysis of QUIC.
 As a result, I only used this configuration to test the correct partial set-up required for the following simulation environment.
 
 To elevate these issues, I used a separate intermediate dedicated machine to introduce traffic perturbations.
 Such a testing environment has already been used in previous studies~\cite{Making_QUIC_Quicker}.
 However, it was a rather complicated set-up process to configure the packet forwarding and network parameters correctly.
 However, as I was not restricted by hardware, I used this option for the final performance analysis tests.
 I used this environment to run integration tests.
 
 Alternatively, I could have used Virtual machines to simulate the tests.
 However, there would have been software related performance penalty required to simulate the guest operating systems.
 
 Similarly, there was a design choice I had to make when selecting a tool that could add traffic perturbations.
 Yang et al.~\cite{Making_QUIC_Quicker} used \texttt{TLEM} network emulator that is suitable for the high throughput network emulation~\cite{Very_high_speed_link_emulation_with_TLEM}.
 \simon{Though TLEM is for "high-speed" networks, does it refer to low latency or high throughput?}
 However, I used \texttt{NetEm} (abbreviation for \enquote{Network Emulator}) tool instead because it was simple to deploy and use.
 Besides that, I measured additional packet delay latency induced by \texttt{NetEm} on the intermediate machine, and it was negligible.
 The overhead measurement results are located in \S~\ref{physical_setup_subsection}.


\section{Starting Point}
As mentioned in the previous sections, this project builds on top of \texttt{ngtcp2} implementation of \texttt{QUIC}.
In other words, relevant networking stack tools such as cryptographic library (e.g. \texttt{OpenSSL}), benchmarking tool (e.g. \texttt{h2load}), custom \texttt{HTTP} layers (e.g. \texttt{nghttp2} and \texttt{ngtcp3}) have already been built and integrated. 
Moreover, in this project I used inspiration from Yang et al.~\cite{Making_QUIC_Quicker} when configuring the testing environment of \texttt{QUIC}.
In particular, I replicated their physical and architectural layout of components required to simulate different conditions of the network (see section \ref{physical_setup_subsection}).

The concepts of \texttt{QUIC} were introduced in the Part IB \enquote{Computer Networking}, and Part II \enquote{Principles of Communications} courses.
In addition, fundamentals of C and C\texttt{++} were covered in \enquote{Programming in C and C\texttt{++}} course. 
Besides that, I used C\texttt{++} in my summer internship.

Throughout the project, the focus of the dissertation changed from hardware to software.
Hence, hardware offloading oriented code (e.g. \texttt{NetFPGA} codebase) and tools (e.g. \texttt{Xilinx/Vivado} tools for programming with FPGAs) used at the beginning of the project had little utility for the remaining part of the dissertation.
In addition, I did not end up using the high-speed network emulator called \texttt{TLEM}~\cite{TLEM_tool} because the existing network emulation tool called \texttt{NetEm} did not seem to add a noticeable performance overhead for the selected testing workloads.


\section{Software Engineering Techniques and Tools} \label{Software_Engineering_Techniques_and_Tools}
    To track the code changes, I used \texttt{Git} source control tool.
    In particular, I used \texttt{GitHub} to store forked repositories of \texttt{ngtcp2}, \texttt{openssl}, \texttt{nghttp2} and \texttt{nghttp3}.
    In addition, the dissertation document was written using the online \LaTeX\  editor Overleaf\footnote{\url{https://www.overleaf.com/}} and changes were also committed to a separate repository on \texttt{GitHub}.
    Furthermore, I used different coding environments.
    At first, I worked with \texttt{NetBeans} IDE.
    But as I was extensively analysing existing code written by other programmers (i.e. \texttt{ngtcp2} and its related components), I needed an effective tool to search through the codebase.
    Hence, I opted for \texttt{Visual Studio Code} IDE which also had a convenient tool to compare \texttt{git} changes.
    
    
    Throughout the project, I employed \texttt{Agile} software development practices.
    For instance, for performance evaluation, I had to configure testing machines using repetitive commands but typing these instructions by hand is a laborious and error-prone process.
    As a result, after detecting an error caused by an incorrect configuration, I usually augmented the \texttt{bash} setup script to set missing parameters automatically.
    Furthermore, I had Weekly progress updates with several other Part II students.
    During these virtual \enquote{stand-up meetings} I had an opportunity to reflect on project-related issues.
    To record my daily progress, I used a logbook, stored in the \texttt{Google Docs}.
    Besides that, I used an application called \texttt{Trello}\footnote{\url{https://trello.com/en-GB}} to group, track and reorder subtasks according to their importance (see Figure~\ref{fig:Trello_board}).

    \begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\textwidth]{figs/Trello_board.PNG}
    \caption{\texttt{Trello} board of important tasks}
    \label{fig:Trello_board}
    \end{figure}
    
    During the winter vacation, I returned back home, so I had made the necessary steps beforehand to be able to have remote access to the specialised hardware.
    In particular, I first enabled permanent remote access to the testing machines using remote desktop access software called \texttt{TeamViewer}\footnote{\url{https://www.teamviewer.com/en/}}.
    It allowed me to access the aforementioned machines outside the college network.
    Then, as shown in Figure~\ref{fig:setup_map}, I added additional redundant backup \texttt{ssh} connections between the testing computers to improve the resilience of the system in case one of the \texttt{TeamViewer} links failed.
    Finally, to further mitigate potential risks, I had two emergency contacts (two non-technical students from the same college) who were granted permission to enter my room and access the machines inside it in case of an accident.
    This last step turned out to be useful because I was able to ask my fellow students to turn off the testing machines before the college network was shut down for maintenance operations.
    

    \begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\textwidth]{figs/Setup map.png}
    \caption[Contingency plan to work remotely]{Contingency plan to work remotely}
    \label{fig:setup_map}
    \end{figure}

    Besides that, I used different communication channels to discuss configuration and technical issues with other \texttt{QUIC} developers.
    At first, I used \texttt{GitHub} to raise issues.
    Then, I started using a direct messaging application called \texttt{Slack}\footnote{\url{https://slack.com/intl/en-gb/}} 
    and it improved the speed of communications substantially.

    Furthermore, after winter vacation, I brought an additional laptop from home to improve the development speed.
    This computer was used as a sandbox to test different implementations of \texttt{QUIC} before deploying them to the main machines.
    Before then, I had a problem that testing machines had pre-configured software which used incompatible settings with the configuration parameters required for \texttt{QUIC} implementations (e.g. existing software stack used older OS version).
    However, with the help of the additional laptop, I was able to verify the correctness of \texttt{ngtcp2}, a selected implementation of \texttt{QUIC}.
    Besides that, I ensured the correctness of the \textit{null} encryption of \texttt{ngtcp2} by writing and running several integration tests.
    In particular, these tests check whether or not the received \texttt{HTML} file is identical to the transmitted \texttt{HTML} file.
    
    Finally, for some aspects of the project, I used non-standard practices.
    For example, I ran testing machines with root privileges.
    On the one hand, such a configuration would be insecure for the general computer user. On the other hand, using root privileges by default allowed me to avoid frequent issues with insufficient permissions as I had to run configuration commands which required top-level permissions multiple times.
    
    

\chapter{Implementation}
% ------------------------------------------------
% TODO

%This chapter should describe what was actually produced: the programs which were written, the hardware which was built or the theory which was developed. Any design strategies that looked ahead to the testing stage should be described in order to demonstrate a professional approach was taken.

%Descriptions of programs may include fragments of high-level code but large chunks of code are usually best left to appendices or omitted altogether. Analogous advice applies to circuit diagrams or detailed steps in a machine-checked proof.

%The implementation chapter should include a section labelled "Repository Overview". The repository overview should be around one page in length and should describe the high-level structure of the source code found in your source code repository. It should describe whether the code was written from scratch or if it built on an existing project or tutorial. Making effective use of powerful tools and pre-existing code is often laudable, and will count to your credit if properly reported. Nevertheless, as in the rest of the dissertation, it is essential to draw attention to the parts of the work which are not your own. 

%It should not be necessary to give a day-by-day account of the progress of the work but major milestones may sometimes be highlighted with advantage.


% FROM: https://www.cst.cam.ac.uk/teaching/part-ii/projects/assessment
%Contribution to the field.
%Application of extra-curricular reading and original interpretation of previous work from academia or industry.
%Challenging goals and substantial deliverables with excellent selection and application of appropriate mathematical, scientific and/or engineering techniques.
%Clear and justified repository overview.
%At most minor faults in execution or understanding.
% ------------------------------------------------


In this chapter, I first present my work required to disable encryption of \texttt{QUIC}.
Then, I describe the actual software engineering tasks that I completed to measure \texttt{QUIC} performance consistently and correctly.


\section{Implementation Milestones}
Firstly, I identified a suitable implementation of \texttt{QUIC}.
Then, I had to disable the packet header and payload encryption.
After that, I configured a physical testing environment to run performance analysis tests (see Figure~\ref{fig:Physical_testing_environment}).
Finally, I wrote several \texttt{bash} scripts to analyse \texttt{QUIC's} behaviour in different conditions. 

\section{Repository Overview} 

    \begin{figure}[ht]
\begin{forest}
  for tree={
    font=\ttfamily,
    grow'=0,
    child anchor=west,
    parent anchor=south,
    anchor=west,
    calign=first,
    inner xsep=7pt,
    edge path={
      \noexpand\path [draw, \forestoption{edge}]
      (!u.south west) +(7.5pt,0) |- (.child anchor) pic {folder} \forestoption{edge label};
    },
    before typesetting nodes={
      if n=1
        {insert before={[,phantom]}}
        {}
    },
    fit=band,
    before computing xy={l=15pt},
  }  
[top level folder
  [unencrypted\_stack
      [{openssl, ngtcp2, nghttp2, nghttp3}
      ]
  ]
  [Original encrypted stack: {openssl, ngtcp2, nghttp2, nghttp3}
  ]
  [Part-II-dissertation
    [testing-scripts
    ]
    [testing-results
    ]
    [testing-libraries
    ]
  ]
  [Additional evaluation tools: {FlameGraph, iperf, msquic\_folder}
  ]
  [Part-II-dissertation-document
  ]
]
\end{forest}

    \caption[High-level directory structure of the project]{High-level directory structure of the project}
    \label{fig:High_level_directory_structure_of_the_project}
    \end{figure}

My project structure is illustrated in Figure~\ref{fig:High_level_directory_structure_of_the_project}.
I implemented support for \textit{null} encryption in the cloned repository of \texttt{ngtcp2} that is located in \texttt{unencrypted\_stack} folder.
This folder also contains a forked networking stack required to run \texttt{ngtcp2}. 
In particular, I worked on separate \texttt{OpenSSL} (embedded security layer), \texttt{ngtcp2} (transport layer), \texttt{nghttp2}, and \texttt{nghttp3} (application layers) repositories.
This software stack is split into multiple repositories because it is implemented in a modular fashion.
For instance, adopted \texttt{OpenSSL} library is used by other \texttt{QUIC} implementations, such as \texttt{MsQuic}\footnote{\url{https://github.com/quictls/openssl/tree/OpenSSL_1_1_1k+quic}}.
\texttt{ngtcp2} uses code from \texttt{OpenSSL} repository, while \texttt{nghttp2} uses code from both \texttt{OpenSSL},  \texttt{ngtcp2}, and  \texttt{nghttp3} repositories because they are used to build an experimental performance analysis tool called \texttt{h2load} that can compare \texttt{HTTP/3} and \texttt{HTTP/2} throughput.
It is important to mention that \texttt{h2load} was already provided in the original \texttt{nghttp2} repository.


I used the original encrypted \texttt{ngtcp2} stack (see Figure~\ref{fig:High_level_directory_structure_of_the_project}) during the development process to compare its behaviour with my implementation.
In addition, I added unit and integration tests of \textit{null} encryption to the \texttt{ngtcp2} repository located in \texttt{unencrypted\_stack} folder. 






Performance analysis and evaluation scripts are placed under \texttt{Part-II-dissertation} folder.
I added instructions on how to compile and run the software stack required for this project to the \texttt{README} file on the \texttt{Part-II-dissertation} repository.
Using the provided instructions, I also added build scripts to automate the process of building \texttt{ngtcp2} software stack to ensure that the same build flags are used consistently.
\texttt{Part-II-dissertation} folder also contains \texttt{testing-scripts} folder that has several different automated evaluation tests.
For example, it contains \texttt{iperf} tests that were used to measure \texttt{TCP} and \texttt{UDP} throughput.
Some of the tests used \texttt{ssh} commands to control the intermediate machine.
To reuse the code of common testing functions, I moved them to the \texttt{testing-libraries folder}.
Results of several manually completed tests are stored in the \texttt{testing-results folder}.
Some \texttt{QUIC} behaviour analysis tests use additional evaluation tools (e.g., \texttt{FlameGraph}, \texttt{iperf}, \texttt{MsQuic}).
The most of the work was done in \texttt{unencrypted\_stack/ngtcp2}, \texttt{unencrypted\_stack/nghtt2} and \texttt{testing-scripts} sections.







\subsection{Software Licences} 
The vast majority of software used in this project was released under permissive \texttt{MIT}\footnote{\url{https://github.com/ngtcp2/nghttp3/blob/main/COPYING}, \\  \url{https://github.com/ngtcp2/ngtcp2/blob/main/COPYING}, \\
\url{https://github.com/microsoft/msquic/blob/main/LICENSE}} or \texttt{Apache 2.0}\footnote{\url{https://github.com/quictls/openssl/blob/openssl-3.0.0-alpha15\%2Bquic/LICENSE.txt}} licences.
As required by \texttt{Apache 2.0} licence, I declare that no changes were added to the \texttt{OpenSSL} library (i.e., adjustments of cryptographic functions were implemented in \texttt{ngtcp2} and \texttt{nghttp2} repositories).
In addition, some other auxiliary repositories, such as \texttt{nghttp3} or \texttt{MsQuic} were not modified, so I did not submit them for assessment.



% \simon{\texttt{nghttp2} does not have an associated licence\footnote{\url{https://github.com/nghttp2/nghttp2/blob/master/LICENSE}}}





















\section{Preparation Scripts}
As I have already mentioned, I wrote several scripts to prepare testing machines for performance analysis.
These commands are part of \texttt{\textasciitilde/.zshrc} and \texttt{\textasciitilde/.bashrc} files (see Appendix~\ref{preparation_script_from_zshrc}) meaning that preparation scripts are executed every time a new terminal is opened.
Preparation scripts checked connectivity of network links by sending \texttt{ping} messages between network interfaces used for testing.
Furthermore, these scripts were used to configure virtual namespaces to separate the server from the client so that packets from the same machine would be forced to leave the physical network interface.
The automatic preparation scripts improved development speed because certain repetitive tasks were automated.
Furthermore, the testing environment setup became more predictable, allowing performance tests to generate more reproducible results.

















\section{Using \textit{Null} Encryption}

As mentioned in \S~\ref{QUIC_details_section}, \texttt{QUIC} uses encryption by default.
It encrypts the payload of the packet and some fields in the packet headers.
A simplified high-level overview of \texttt{QUIC} packet encryption and decryption is illustrated in Figure~\ref{fig:Cryptographic_procedures_of_quic}.
At first, the sender encrypts the \texttt{QUIC} packet payload by using 16-byte long authentication tag~\cite[Section 5]{ietf-quic-tls-32}. 
The authentication tag depends on the destination connection ID, initial salt value and other cryptographic parameters~\cite[Section 5]{ietf-quic-tls-32}.
In this stage, the produced ciphertext is 16 bytes longer than the provided plain text because the ciphertext has to accommodate both the plain text and the authentication tag~\cite[Section 5.3]{ietf-quic-tls-32}.
Then, the packet header is encrypted with a 5-byte long mask.
This mask is derived from the header protection key and several sampled bytes from the ciphertext that was produced in the earlier steps~\cite[Section 5]{ietf-quic-tls-32}. 
%  \texttt{QUIC} packet protection key is   key so they are different
Header encryption is performed by \texttt{XORing} relevant packet header fields (e.g., packet number) with the associated header encryption mask~\cite[Section 5]{ietf-quic-tls-32}.
After that, the encrypted packet is sent over the potentially insecure channel.
To decrypt the contents of \texttt{QUIC} packets receiver has to do the steps by first decrypting the header and then the payload.

    \begin{figure}[H]
    \centering
    \includegraphics[width=0.97\textwidth]{figs/Cryptographic_procedures_of_quic.png}
    \caption[Original process of encrypting, transferring and decrypting \texttt{QUIC} packet]{Original process of encrypting (e.g., steps 1 and 2), transferring  (e.g., steps 3) and decrypting (e.g., steps 4 and 5) \texttt{QUIC} packet. Diagram summarises relevant sections of \texttt{IETF} \texttt{QUIC-TLS} draft~\cite[Section 5]{ietf-quic-tls-32}}
    \label{fig:Cryptographic_procedures_of_quic}
    \end{figure}


However, \texttt{QUIC} encryption and decryption operations are computationally expensive and, under certain circumstances, they may form a performance bottleneck of \texttt{QUIC}~\cite{Making_QUIC_Quicker}.
To validate this claim, I had to measure the impact of cryptographic operations on \texttt{QUIC's} performance by comparing the throughput between the two server-client pairs.
One pair had to use standard encryption and decryption procedures, while another had to exchange unencrypted packets.
I achieved this by implementing logic that enacted a \textit{null} encryption in \texttt{ngtcp2} (see Figure~\ref{fig:my_implementation_of_cryptographic_procedures_of_quic}).
I remove the encryption of \texttt{QUIC} packets by performing early returns in associated functions.
However, to preserve the payload format, I had to pad unencrypted payloads with sixteen zeroes.
Usually, the header encryption key is generated by sampling the first few hundred bytes of the encrypted packet~\cite[Section 5.4.1]{ietf-quic-tls-32}. 
In order to avoid this and ensure that packets are also completely transparent, I disabled header encryption by replacing the header encryption mask with a string of zeroes.
But exclusive-or operation performed on a sequence of zeroes preserves the original sequence of bits intact.
On the one hand, packet headers are still \texttt{XORed} with a fixed 5-byte long mask.
On the other hand, my implementation avoids the sampling procedure required to generate the header encryption key in the first place.
Alternatively, I could have even removed the function calls used to encrypt \texttt{QUIC} packet headers.
However, \texttt{QUIC} is a complex protocol, so removing these functions may have had additional unintended side effects that could have led to protocol errors and additional (unnecessary) debugging.
Finally, \texttt{QUIC} headers are relatively small compared with the packet payloads, so further performance optimisations dealing with header encryption would have provided only limited performance improvements.


    \begin{figure}[H]
    \centering
    \includegraphics[width=0.97\textwidth]{figs/my_implementation_of_cryptographic_procedures_of_quic.png}
    \caption[My implementation of disabled encryption]{My implementation of disabled encryption}
    \label{fig:my_implementation_of_cryptographic_procedures_of_quic}
    \end{figure}

At first, I used a naive implementation to copy relevant packet bits.
However, as memory copying took a proportionally long time to complete, I used built-in functions instead.
For instance, \texttt{memmove} function from \texttt{string.h} library runs significantly faster than my simple algorithm that copies one symbol at a time; after running 100 experiments that copy arrays with 1000000 symbols, my naive implementation took $5.001 \pm 0.029$ milliseconds, while \texttt{memmove} took $0.151 \pm 0.002$ milliseconds.
Hence, this performance analysis allowed me to identify potential bottlenecks of my implementation and thus allowed me to improve my cryptographic operations in a systematic way.





My final implementation of \textit{null} encryption has room for improvements.
For instance, every encrypted \texttt{ngtcp2} packet uses additional 16 bytes to store the encryption header.
However, in the null-encryption scenario, these additional fields are not required, but my implementation of disabling encryption does not use them, meaning that 16 unused bytes are transferred in a single \texttt{QUIC} packet.
In contrast, \texttt{MsQuic} does have a mechanism allowing unencrypted \texttt{QUIC} packets to re-use these additional 16 bytes for data transfer.



\section{Performance Analysis Scripts}

In order to measure the performance of the \texttt{QUIC} protocol, I ran several automated scripts on the testing computer (see Figure~\ref{fig:Physical_testing_environment}).
In particular, I used Python and/or Bash scripts to configure the testing environment and initiate the \texttt{QUIC} server-client pair.
In addition, to control the networking conditions of the simulated network, the scripts on the testing machine used \texttt{ssh} (abbreviation of Secure Shell Protocol) connection to control the intermediate machine.
Later on, Python scripts saved, aggregated and visualised the results of the experiments.
The use of scripting languages, in this case, was acceptable because performance measurements required quick prototyping.
Furthermore, code maintainability was not the major design goal of this project.



\section{Testing Scripts}

To check the correctness of disabled \texttt{QUIC} encryption, I added both unit and integration tests.
Unit tests ensure that the original text is equal to the encrypted and then decrypted text.
Integration tests transfer large \texttt{HTML} file between the \texttt{QUIC} client and server via the intermediate machine (see Figure~\ref{fig:Physical_testing_environment}).
After that, the sender's and receiver's file contents are compared.


\section{Backup Plans}
As I have already mentioned in \S~\ref{Software_Engineering_Techniques_and_Tools}, I used \texttt{GitHub} to store a backup copy of the software.
In addition, I shared configuration instructions (see Appendix~\ref{preparation_script_from_zshrc}) between the two computers that were used in the experiment.



\chapter{Evaluation}
% ------------------------------------------------
%This is where Assessors will be looking for signs of success and for evidence of thorough and systematic evaluation. Sample output, tables of timings and photographs of workstation screens, oscilloscope traces or circuit boards may be included. Care should be employed to take a professional approach throughout. For example, a graph that does not indicate confidence intervals will generally leave a professional scientist with a negative impression. As with code, voluminous examples of sample output are usually best left to appendices or omitted altogether.

%There are some obvious questions which this chapter will address. How many of the original goals were achieved? Were they proved to have been achieved? Did the program, hardware, or theory really work?

%Assessors are well aware that large programs will very likely include some residual bugs. It should always be possible to demonstrate that a program works in simple cases and it is instructive to demonstrate how close it is to working in a really ambitious case.
% ------------------------------------------------



\section{Setup}

As this project builds on the ideas presented in~\cite{Making_QUIC_Quicker}, I thought that it would be reasonable to replicate a similar testing environment to obtain comparable results.

As a side note, different tools and vendors use different notation to represent throughput and memory units.
To make experiments consistent, I used SI units (1~Gbyte = 1000~MBytes = 1~000~000~KBytes = 1~000~000~000~Bytes and 1~Gbit = 1000~Mbits = 1~000~000~Kbits = 1~000~000~000~bits).
Furthermore, where applicable, I added additional logic to represent throughput measurements as Mbits/second, completion time in terms of milliseconds and memory in terms of MBytes.

\subsection{Logical Setup}
To test a simple single flow performance, it is sufficient to have a single \texttt{QUIC} server-client pair, as depicted in Figure~\ref{fig:Logical_testing_environment}.
To measure \texttt{QUIC} performance under different conditions, a network emulator between the \texttt{QUIC} client and the \texttt{QUIC} server has to be added.

    \begin{figure}[ht]
    \centering
    \includegraphics[width=0.95\textwidth]{figs/Logical_testing_environment.png}
    \caption{Logical configuration of testing environment}
    \label{fig:Logical_testing_environment}
    \end{figure}

\subsection{Physical Setup} \label{physical_setup_subsection}


    In the ideal scenario, it might be the best to use three dedicated machines for performance measurements, as shown in the previous section.
    However, I decided to use a slightly different physical layout (see Figure~\ref{fig:Physical_testing_environment}).
    In this environment, one machine (A) hosts client-server pair, while another computer (B) acts as a network emulator.
    Software parameters of the testing machine are shown in Table~\ref{software_parameters_of_testing_machine}.
    The physical configuration differs from the logical configuration because I wanted to replicate \texttt{QUIC} testing conditions used in other experiments~\cite{Making_QUIC_Quicker}.
    One advantage of having both server and client running on the same machine is that the common system clock can perform measurements more accurately. 
    

    \begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\textwidth]{figs/Physical_testing_environment.png}
    \caption[Physical configuration of the testing environment]{Physical configuration of the testing environment. This scheme is an adaptation of the configuration environment used in ~\cite{Making_QUIC_Quicker}}
    \label{fig:Physical_testing_environment}
    \end{figure}
    
    \begin{table}[ht]
        \centering
        \begin{tabular}{|l|l|}
        \hline
        OS     & Ubuntu 20.04.2.0 LTS (Focal Fossa) \\ \hline
        Kernel & 5.4.0-72-generic                   \\ \hline
        GCC    & 10.2.0                             \\ \hline
        Python & Python 2.7.18 and Python 3.8.5     \\ \hline
        \end{tabular}
        
        \caption[Software parameters of the testing machine]{Software parameters of the testing machine. Idea to add this table is taken from~\cite{quic_vs_tcp}.}
        \label{software_parameters_of_testing_machine}
    \end{table}
    
    As shown in Figure~\ref{fig:Physical_testing_environment}, both \texttt{QUIC} client and server operate on different virtual network namespaces which do not have shared point-to-point links.
    Client and server are assigned different network subnets.
    Subnets use private \texttt{IPv4} addresses~\cite{rfc1918}.
    The purpose of these addresses is to prevent experiments and the college network from interfering with one another.
    An intermediate machine (B) is configured as the default gateway for both virtual network namespaces.
    As a result, all the packets between the \texttt{QUIC} server and client have to travel via an intermediate machine (B).
    Here, depending on the experiment, network perturbations (e.g. packet loss, delay, reordering) may be introduced.
    In particular, to simulate different networking conditions I used \texttt{NetEm} tool (abbreviation of \enquote{Network Emulator})\footnote{http://manpages.ubuntu.com/manpages/trusty/man8/tc-netem.8.html}.
    The small implementation detail of \texttt{NetEm} is that by default, it introduces traffic perturbations for the packets leaving the network interface but not the incoming packets~\cite{Ubuntu_Manpage_NetEm}.
    Hence, to simulate symmetrical network conditions, I had to specify \texttt{NetEm} rules for both interfaces of the intermediate machine.
    For this reason, in other diagrams, I represent \texttt{Network Emulator} as separated into two sections (see Figure~\ref{fig:Wireshark_separation}).
    
    
    IP forwarding is enabled on the intermediate machine (B), redirecting \texttt{QUIC} packets from one subnet to another.
    The correctness of the experimental environment was tested by inspecting transit packets on machine B.



    The intermediate machine adds only limited overhead.
    I derived these conclusions by comparing round-trip-times (using \texttt{ping} tool) and the maximum available \texttt{UDP} throughput (using \texttt{iperf3} tool) under two different conditions.
    At first, I measured performance when packets travel from one physical interface of machine A to its another physical interface via machine B.
    Then, I performed identical tests but without the intermediate machine meaning that packets were still forced to go from one physical interface to another via a loopback cable.
    Results were as follows: after running ten \texttt{iperf3} experiments, the average \texttt{UDP} throughput sending \texttt{UDP} packets containing 1448 data bytes via machine B was $4.11$ Gbits/sec.
    Alternatively, \texttt{UDP} throughput via loopback cables was $4.22 \pm 0.03$ Gbits/sec or $364389.8$ packets per second.
    However, by increasing \texttt{iperf3} buffers from 1460 bytes for UDP, to more than about 65000 bytes, I improved loopback \texttt{UDP} throughput to 9.93 Gbits/sec.
    In either cases, throughput was measured on the receiver side.
    
    Similarly, \texttt{TCP} throughput via intermediate was 9.24 Gbits/sec and via loopback cable it was 9.41 Gbits/sec.


    Similarly, after introducing the intermediate machine, the \texttt{RTT} increased from
    $0.118 \pm 0.026$ ms to $0.262 \pm 0.065$ ms.
    Here, each result was obtained after running ten \texttt{ping} experiments.
    




\section{Parameter Tuning}

\subsection{IP Version}

To make results comparable with older studies, I decided to disable IPv6 in the testing environment.
The number of variable parameters is reduced by specifying a particular IP version.
Fortunately, at the time of writing, according to Google's IPv6 adoption tracker~\cite{IPv6_Adoption_Statistics}, only a third of all Internet users (30-35 \% to be precise) use IPv6 to connect to Google's services.
Hence, analysis involving IPv4 is still relevant today.

However, I could not run \texttt{MsQuic} tools with IPv6 disabled, so at the end of the project I had to revert this setting.
Fortunately, using \texttt{Wireshark} I observed that \texttt{MsQuic} also uses IPv4 by default, meaning that results were comparable.

\subsection{Hyper-threading}\label{Hyperthreading_Subsection_Tag}

\texttt{Hyper-threading} is a thread allocation technique that allows two threads to run on the same physical core at the same time~\cite[page~ 23]{hyperthreading_book}.
Another name for \texttt{hyper-threading} is \texttt{simultaneous multi-threading}.
On the testing machines \texttt{hyper-threading} at first was implemented by assigning two virtual cores for every physical core (see Figure~\ref{fig:topology_with_hyperthreading}).
For instance, in this diagram core \enquote{L \#0} has two \enquote{processing units} marked as \enquote{PU L\#0} and \enquote{PU L\#1}.

    \begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figs/topology_with_hyperthreading.png}
    \caption[Memory topology of machine A when \texttt{hyper-threading} is turned on]{Memory topology of machine A when \texttt{hyper-threading} is turned on (Figure generated by \texttt{lstopo} command)}
    \label{fig:topology_with_hyperthreading}
    \end{figure}

As stated in~\cite{hyperthreading_book}, this means that threads allocated to such virtual cores (also marked as \enquote{processing units} in the diagram above) would share the same physical resources (e.g. private L1 cache entries).
This is problematic for precise experiments.
On the one hand, a neighbouring thread might pre-fetch required data to the cache, but on the other hand, this thread might pollute the cache, thus hurting the performance of the thread which is being measured.
Hence, to reduce the potential performance variability, I had to disable \texttt{hyper-threading} on the testing machines.
The final version of the architectural topology is shown in Figure~\ref{fig:memory_topology}. 
Correct configuration of \texttt{hyper-threading} was tested using different commands, such as \texttt{lscpu} and the aforementioned \texttt{lstopo}. 

    
    
\subsection{Separated Cores} \label{SeparatedCores_Subsection_Tag}
To make performance tests even more predictable, I had to assign exclusive physical cores to the \texttt{QUIC} client and server using \texttt{taskset}\footnote{https://man7.org/linux/man-pages/man1/taskset.1.html} tool.


In particular, the testing machine from Figure~\ref{fig:Physical_testing_environment} has four physical cores (see Figure~\ref{fig:memory_topology}) and I assigned \texttt{ngtcp2} server to the first core (marked as \enquote{P\#0} in Figure~\ref{fig:memory_topology}).
\texttt{QUIC} client (i.e. \texttt{ngtcp2} or \texttt{h2load} client) was assigned the second core (marked as \enquote{P\#1}).
Finally, the remaining two cores (marked as \enquote{P\#2} and \enquote{P\#3}) were assigned to all the remaining system processes.
Robert Love refers to this technique of assigning particular processes to particular cores as \texttt{hard CPU affinity}~\cite{CPU_Affinity}.
Core pinning is important for performance analysis because it removes the overhead of reallocating processes from one core to another by reducing the frequency of cache misses~\cite{Tuning_10Gb_network_cards_on_Linux}.

    \begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figs/memory_topology.png}
    \caption[Memory topology of machine A  when \texttt{hyper-threading} is turned off]{Memory topology of machine A  when \texttt{hyper-threading} is turned off (Figure generated by \texttt{lstopo} command)}
    \label{fig:memory_topology}
    \end{figure}

In addition, to evaluate the impact of core pinning, I measured throughput of \texttt{QUIC} under two different conditions by sending a file of the specified size over the intermediate machine.
At first, I allocated both \texttt{QUIC} server and client to the same core (see the red dashes in Figure~\ref{fig:Throughput_via_A-to-B-to-A_MTU=1500}).
Then, I performed identical throughput analysis tests with separated cores meaning that \texttt{QUIC} server and client resided on the two different cores.
As far as testing conditions are concerned, the intermediate machine was not configured to introduce additional explicit traffic perturbations.

Blue symbols in Figure~\ref{fig:Throughput_via_A-to-B-to-A_MTU=1500} demonstrate that core pinning almost doubled the maximum \texttt{QUIC} throughput for the transfer of large files (above 100MBytes).
However, the impact of core pinning is not substantial when transferring small files.
One explanation of this could be that the transmission time of small files is dominated by the overhead of the networking stack and the transmission time of a single packet.
In contrast, transfer of large files is a data intensive operation.
Hence, two data-intensive processes that run on the same core could compete for the same cache resources.
In addition, Table~\ref{Standard_deviation_of_throughput_measurements} demonstrates that \texttt{h2load's} throughput was almost always more variable when both the client and the server used the same core.
Because of the performance improvements and more consistent results, I use core pinning by default in all the experiments (if not stated otherwise).

    \begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figs/QUIC Throughput when packets go via machine B (A1-to-B-to-A2).png}
    \caption{Impact of core pinning for \texttt{QUIC} throughput measured with \texttt{h2load}}
    \label{fig:Throughput_via_A-to-B-to-A_MTU=1500}
    \end{figure}
% TODO
\simon{TODO:If there is time, augment this diagram with iperf3 measurements on different cores}





\begin{table}[H]
\begin{tabular}{|c|r|r|r|r|r|r|r|r|r|}
\hline
\begin{tabular}[c]{@{}c@{}}Number \\ of transferred bytes\end{tabular}                              & \multicolumn{1}{c|}{10} & \multicolumn{1}{c|}{100} & \multicolumn{1}{c|}{1k} & \multicolumn{1}{c|}{10k} & \multicolumn{1}{c|}{100k} & \multicolumn{1}{c|}{1M} & \multicolumn{1}{c|}{10M} & \multicolumn{1}{c|}{100M} & \multicolumn{1}{c|}{1G} \\ \hline
\begin{tabular}[c]{@{}c@{}}throughput \\ standard deviation \\ (using the same core)\end{tabular}   & 0.010                   & 0.011                    & 0.040                   & 0.749                    & 2.010                     & 4.361                   & 2.717                    & 1.409                     & 1.426                   \\ \hline
\begin{tabular}[c]{@{}c@{}}throughput \\ standard deviation \\ (using different cores)\end{tabular} & 0.003                   & 0.002                    & 0.025                   & 0.470                    & 3.264                     & 5.557                   & 3.593                    & 1.277                     & 0.669                   \\ \hline
\end{tabular}
    \caption{Standard deviation of throughput measurements that are shown in Figure~\ref{fig:Throughput_via_A-to-B-to-A_MTU=1500}}
    \label{Standard_deviation_of_throughput_measurements}
\end{table}




\subsection{GSO}
    Generic Segmentation Offload (abbreviated GSO) is a software technique that groups system calls issued by user-space applications to send \texttt{UDP} packets~\cite{segmentation-offloads, Tuning_10Gb_network_cards_on_Linux, segmentation-offloading-with-wireshark-and-ethtool}.
    As a result, the kernel has to make fewer context switches to segment the data into Ethernet packets before sending them over the network~\cite{accelerating-udp-packet-transmission-for-quic}.
    
    To measure GSO impact for \texttt{QUIC} throughput, I compared \texttt{QUIC} throughput between the \texttt{ngtcp2} server and \texttt{h2load} client.
    I performed two groups of experiments with different file sizes.
    At first, I disabled Generic Segmentation Offload both on the server and client sides.
    Then, I repeated the same experiments with enabled GSO.
    I repeated every experiment ten times.
    Throughput measurements are left in the appendix (see Table~\ref{GSO_impact_measurement_results}).
    But in general, Generic Segmentation Offload improved \texttt{QUIC} throughput considerably.
    For instance, when transferring 1~Gbyte amount of data, \texttt{QUIC's} throughput increased from $418.44 \pm 3.21$ Mbits/s to $663.06 \pm 0.98$ Mbits/s.
    However, in real networks, GSO could contribute to the more bursty traffic patterns and thus increase the rate of dropped packets~\cite{accelerating-udp-packet-transmission-for-quic}.
    Nevertheless, GSO is used in all the remaining tests by default because it provided significant performance improvements.


\subsection{Maximum Transmission Units}
  MTU (abbreviation of Maximum Transmission Unit) specifies the maximum payload of the Ethernet packet.
  The standard Ethernet MTU size is 1500 bytes~\cite{The_Ethernet_Frame_Payload_Size_and_Its_Effect_on_IPv4_and_IPv6_Traffic}. 
  Murray et al. suggested that default Maximum Transmission Units have relatively high overhead~\cite{Large_MTUs_and_Internet_Performance}.
  For instance, IPv4 packet headers are usually 20 bytes long~\cite{FallKevinR2012TiV1}, and TCP headers take 20-32 bytes~\cite{Large_MTUs_and_Internet_Performance}.
  Hence, about 2.67\%-3.47\% of the standard Ethernet packet payload are wasted.
  In contrast, these per-packet overheads could be reduced by using larger Ethernet frames.
  For example, the MTU of so-called jumbo frames is 9000 bytes meaning that per-packet header overhead is reduced to just 0.44\%-0.57\%.
  However, the major improvement of jumbo frames is that they can reduce CPU workload by reducing the number of interrupts required to process Ethernet packets~\cite{Extended_rame_Sizes_for_Next_Generation_Ethernets}.
  
  
   Table~\ref{fig:Impact_of_Jumbo_frames_for_ngtcp2_throughput} summarises tests that measured the impact of different MTU sizes.    
    In particular, this experiment measured \texttt{QUIC} throughput between the server and the client when they were pinned to two different cores on the same machine, and their packets travelled via an intermediate machine (as demonstrated in Figure~\ref{fig:Physical_testing_environment}).
    Furthermore, a server used a single thread, only a single stream was used, a single data transfer was performed for each experiment, each experiment ran for ten seconds, and each test was repeated ten times.
    According to Table~\ref{fig:Impact_of_Jumbo_frames_for_ngtcp2_throughput}, \texttt{QUIC} throughput could be improved by increasing the size of Ethernet frames.
    However, this improvement is only applicable when transferring large (e.g., 1 GByte) files.
    It is important to note that in these tests I used \texttt{nuttcp}, a recommended \texttt{UDP} throughput measurement tool~\cite{network-troubleshooting-tools_nuttcp}.
    For this experiment I decided to use \texttt{nuttcp} instead of \texttt{iperf3} because  \texttt{UDP's} throughput measured with \texttt{iperf3} was significantly lower than \texttt{TCP's} throughput under the same networking conditions.
    Furthermore, to reach the maximum \texttt{UDP} throughput I had to increase window size to 4 MBytes.
    
    MTUs above 1500 bytes are not prevalent on the Internet~\cite{why-is-ethernet-mtu-1500}.
    Hence, network elements that do not support jumbo frames would either have to drop them or fragment them into smaller packets~\cite{Large_MTUs_and_Internet_Performance}.
    This, in turn, could lead to performance degradation.
    Hence, if not specified otherwise, I used MTU of 1500 bytes in all other experiments.
    
    
  
  
    \begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{figs/UDP throughput using different MTUs and measured with nuttcp (packets travel via the intermediate machine).png}
    \caption{UDP throughput using different MTUs and measured with \texttt{nuttcp} (packets travel via the intermediate machine)}
    \label{fig:UDP_throughput_with_different_MTUs}
    \end{figure}
    
    % \begin{figure}[H]
    % \centering
    % \includegraphics[width=0.97\textwidth]{figs/Iperf3 UDP throughput using different MTUs (localhost).png}
    % \caption{Impact of MTU size for iperf throughput (when packets travel via localhost)}
    % \label{fig:MTU_impact_for_localhost_iperf_throughput}
    % \end{figure}
    
    % \begin{figure}[H]
    % \centering
    % \includegraphics[width=0.9\textwidth]{figs/Iperf3 UDP throughput using different MTUs (A1-to-B-to-A2).png}
    % \caption{Impact of MTU size for iperf throughput (when packets travel via the intermediate machine)}
    % \label{fig:}
    % \end{figure}
  

\begin{table}[H]

    \begin{tabular}{|c|l|r|r|r|r|}
\hline
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Requested \\ file size\end{tabular}} & \multicolumn{1}{c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}MTU \\ (Byte)\end{tabular}}} & \multicolumn{2}{c|}{\begin{tabular}[c]{@{}c@{}}Completion \\ time (s)\end{tabular}}                            & \multicolumn{2}{c|}{\begin{tabular}[c]{@{}c@{}}Throughput\\ (Mbits/s)\end{tabular}}                            \\ \cline{3-6} 
                                                                                & \multicolumn{1}{c|}{}                                                                       & \multicolumn{1}{c|}{mean} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}standard \\ deviation\end{tabular}} & \multicolumn{1}{c|}{mean} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}standard \\ deviation\end{tabular}} \\ \hline
\multirow{3}{*}{1MByte}                                                         & \textbf{1280}                                                                               & 0.022                     & 0.001                                                                              & 370.147                   & 20.865                                                                             \\ \cline{2-6} 
                                                                                & \textbf{1500}                                                                               & 0.021                     & 0.001                                                                              & 380.692                   & 21.364                                                                             \\ \cline{2-6} 
                                                                                & \textbf{9000}                                                                               & 0.024                     & 0.001                                                                              & 333.447                   & 22.875                                                                             \\ \hline
\multirow{3}{*}{1GByte}                                                         & \textbf{1280}                                                                               & 9.180                     & 0.166                                                                              & 871.996                   & 15.050                                                                             \\ \cline{2-6} 
                                                                                & \textbf{1500}                                                                               & 8.674                     & 0.075                                                                              & 922.755                   & 7.899                                                                              \\ \cline{2-6} 
                                                                                & \textbf{9000}                                                                               & 6.834                     & 0.092                                                                              & 1171.184                  & 15.540                                                                             \\ \hline
\end{tabular}

    \centering
    \caption[Impact of Jumboframes for the throughput of \texttt{ngtcp2} completion time required to transfer specified files]{Impact of Jumboframes on the throughput of \texttt{ngtcp2} completion time required to transfer specified files. 
    \simon{Are packets going via loopback?}}
    \label{fig:Impact_of_Jumbo_frames_for_ngtcp2_throughput}
\end{table}
  

  
\subsection{Interrupt Request Queue (IRQ) Balancing} 
Interrupt Request Queue (IRQ) Balancing could improve the general throughput of the testing system~\cite{Tuning_10Gb_network_cards_on_Linux}.
By default, \texttt{IRQ} is enabled, meaning that the operating system aims to distribute the burden of handling interrupts evenly over all the processors~\cite{Tuning_10Gb_network_cards_on_Linux}.
An alternative is to delegate the processing of certain interrupts to specified processors hoping that this would improve the performance of cores that were designated for the \texttt{QUIC} tests by better exploiting cache locality.
However, \texttt{IRQ} did not yield any noticeable effects for the \texttt{UDP} throughput measured by \texttt{nuttcp} so I left \texttt{IRQ} settings as default.
In particular, when sending packets via an intermediate machine for 60 seconds, an average \texttt{UDP} throughput with enabled \texttt{IRQ} was 3690.49 Mbits/second and with disabled \texttt{IRQ} \texttt{UDP} throughput was 3535.07 Mbits/second.
One plausible explanation for this insignificant change could be that the testing environment was already using core pinning meaning that packets were already redirected to particular cores.

\subsection{Size of the Buffers} 

In order to saturate the links, I increased \texttt{nuttcp's} window size to be more than the bandwidth-delay product of the link.
To be precise, the testing environment used 10 Gigabit Ethernet links and the end-to-end delay between the client and the server was $0.262 \pm 0.065$ ms (see \S~\ref{physical_setup_subsection}), meaning that the window size was at least $10^{10}*2.62*10^{-4}$~bytes $= 2.62$~Mbytes.

% TODO
\simon{TODO: 1. Update send and receive buffers according to: https://www.kernel.org/doc/ols/2009/ols2009-pages-169-184.pdf}

\simon{TODO: 2. State that adjusted buffers reduce UDP packet loss rate}
 
 \awm{ what are you writing here? make clear you mean txqueue and rxqueue as appropriate and not the "buffers" normally thought of as resident in the intermediate network devices}
  


\section{Baseline Measurements}


\subsection{Throughput of Parallel \texttt{UDP} Streams} \label{section_Throughput_of_Parallel_UDP_Streams}

As illustrated in Figure~\ref{fig:QUIC_network_stack}, \texttt{QUIC} is built on top of \texttt{UDP}.
I measured throughput of \texttt{UDP} because the performance of \texttt{QUIC} is limited by this underlying protocol (see Table~\ref{fig:UDP_throughput_via_B_using_parallel_streams}).
In particular, I used a testing topology similar to the one shown in  Figure~\ref{fig:Physical_testing_environment}.
However, in this experiment I replaced the \texttt{QUIC} client and the server with the corresponding \texttt{iperf3} client-server pair.
In addition, I did not add any additional packet delay or packet loss on the intermediate machine.
Furthermore, I repeated each individual test ten times and measured both the mean and standard deviation of these samples.
As a side note, the Large Receive Offload is disabled on both the testing and intermediate machines.
















\begin{table}[H]
\begin{tabular}{|r|r|r|r|r|r|}
\hline
\multicolumn{1}{|c|}{\begin{tabular}[c]{@{}c@{}}Requested\\ file size \\ (Mbytes)\end{tabular}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Number of\\ parallel \\ clients\\ (1 request\\ per client)\end{tabular}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Average\\ total\\ throughput\\ measured\\ on the\\ receiver's\\ side (Mbit/s)\end{tabular}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Standard \\ deviation\\ of the\\ throughput\\ sample \\ side (Mbit/s)\end{tabular}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Average\\ total\\ completion\\ time (ms)\end{tabular}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Standard \\ deviation of\\ the completion\\ time sample\\ side (Mbit/s)\end{tabular}} \\ \hline
1                                                                                               & 1                                                                                                                       & 6492.92                                                                                                                                    & 379.66                                                                                                                               & 1.19                                                                                                  & 0.07                                                                                                                               \\ \hline
1                                                                                               & 10                                                                                                                      & 7023.35                                                                                                                                    & 252.09                                                                                                                               & 1.17                                                                                                  & 0.05                                                                                                                               \\ \hline
10                                                                                              & 1                                                                                                                       & 9759.24                                                                                                                                    & 100.66                                                                                                                               & 6.86                                                                                                  & 0.15                                                                                                                               \\ \hline
10                                                                                              & 10                                                                                                                      & 9786.08                                                                                                                                    & 112.75                                                                                                                               & 6.85                                                                                                  & 0.16                                                                                                                               \\ \hline
100                                                                                             & 1                                                                                                                       & 10455.02                                                                                                                                   & 176.66                                                                                                                               & 76.47                                                                                                 & 1.30                                                                                                                               \\ \hline
100                                                                                             & 10                                                                                                                      & 10379.86                                                                                                                                   & 204.23                                                                                                                               & 64.78                                                                                                 & 0.45                                                                                                                               \\ \hline
1000                                                                                            & 1                                                                                                                       & 10642.93                                                                                                                                   & 15.85                                                                                                                                & 801.77                                                                                                & 0.98                                                                                                                               \\ \hline
1000                                                                                            & 10                                                                                                                      & 10628.97                                                                                                                                   & 21.74                                                                                                                                & 659.68                                                                                                & 6.22                                                                                                                               \\ \hline
\end{tabular}
    \centering
    \caption[Impact of parallel streams for \texttt{UDP} throughput]{Impact of parallel streams for \texttt{UDP} throughput. Here packets travel via an intermediate machine.}
    \label{fig:UDP_throughput_via_B_using_parallel_streams}
\end{table}


These experiments aimed to determine the impact of parallel \texttt{UDP} streams for the aggregated throughput of \texttt{UDP}.
As demonstrated in Table~\ref{fig:UDP_throughput_via_B_using_parallel_streams}, adding concurrent \texttt{UDP} clients marginally increased the aggregated goodput when transferring small (i.e., 1~Mbyte) files.
In addition, \texttt{UDP} throughput reached a plateau as transferred file size increased.
This can be explained by the fact that there was a single \texttt{iperf3} server that processed all the packets and that the underlying Ethernet links supported throughput up to 10 Gigabits per second.
Change of \texttt{UDP} throughput in time is shown in Figure~\ref{fig:UDP_throughput_in_time}.
Here throughput was measured at the granularity of 0.1 s.
In this experiment I transferred fixed number of bytes meaning that during the last interval (0.8-0.9 s) the channel could not be fully utilised.
This explains \texttt{UDP} throughput decrease at the end of the measurement. 


    \begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figs/UDP_throughput_in_time.png}
    \caption[Change of \texttt{UDP} throughput in time when a single \texttt{iperf3} client sends \texttt{UDP} traffic to the \texttt{iperf3} server via an intermediate machine]{Change of \texttt{UDP} throughput in time when a single \texttt{iperf3} client sends \texttt{UDP} traffic to the \texttt{iperf3} server via an intermediate machine.}
    \label{fig:UDP_throughput_in_time}
    \end{figure}

\subsection{Throughput of Parallel \texttt{TCP} Streams}

Similarly, Table~\ref{fig:TCP_throughput_via_B_using_parallel_streams} demonstrates \texttt{TCP} throughput measurements using the same testing environment as described in the previous subsection (see \S~\ref{section_Throughput_of_Parallel_UDP_Streams}).
In other words, I repeated each individual experiment ten times, packets travelled via the intermediate machine that was configured not to add any additional packet delay or packet loss.

\simon{Combine these two paragraphs sections}

Table~\ref{fig:TCP_throughput_via_B_using_parallel_streams} shows the effect of increased transferred file size for the throughput of \texttt{TCP}.
Naturally, it takes less time to transfer a small file than the large one.
However, this means that \texttt{TCP} spends most of the time in the \enquote{slow start} phase when sending small files because their transfer time is shorter than the duration of the \enquote{slow start}.
As a result, reported \texttt{TCP}'s throughput when sending small files was low.
In contrast, transferring larger files allowed \texttt{TCP} to discover the maximum sustainable network capacity and thus increase the total throughput.
In addition, the transfer of large files allowed \texttt{TCP} to amortise the overhead of initial \texttt{TCP} handshake.
Hence, when sending 1~GByte of data, \texttt{TCP} was able to approach the maximum link capacity that was 10~Gbits/s.

\enquote{Slow start} effect is partially visible in Figure~\ref{fig:TCP_throughput_in_time} that illustrates \texttt{TCP} throughput in time.
However, resolution of this diagram is limited by the \texttt{iperf3} resolution, which is 0.1 s.




\begin{table}[H]
\begin{tabular}{|r|r|r|r|r|r|}
\hline
\multicolumn{1}{|c|}{\begin{tabular}[c]{@{}c@{}}Requested\\ file size \\ (Mbytes)\end{tabular}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Number of\\ parallel \\ clients\\ (1 request\\ per client)\end{tabular}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Average\\ total\\ throughput\\ measured\\ on the\\ receiver's\\ side (Mbit/s)\end{tabular}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Standard \\ deviation\\ of the\\ throughput\\ sample \\ side (Mbit/s)\end{tabular}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Average\\ total\\ completion\\ time (ms)\end{tabular}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Standard \\ deviation  of\\ the completion\\ time sample\\ side (Mbit/s)\end{tabular}} \\ \hline
1                                                                                               & 1                                                                                                                       & 2245.32                                                                                                                                    & 300.09                                                                                                                               & 2.45                                                                                                  & 0.26                                                                                                                               \\ \hline
1                                                                                               & 10                                                                                                                      & 3376.85                                                                                                                                    & 192.65                                                                                                                               & 1.94                                                                                                  & 0.28                                                                                                                               \\ \hline
10                                                                                              & 1                                                                                                                       & 4220.89                                                                                                                                    & 1364.76                                                                                                                              & 19.55                                                                                                 & 10.02                                                                                                                              \\ \hline
10                                                                                              & 10                                                                                                                      & 7083.99                                                                                                                                    & 383.66                                                                                                                               & 12.76                                                                                                 & 0.76                                                                                                                               \\ \hline
100                                                                                             & 1                                                                                                                       & 7391.08                                                                                                                                    & 1137.97                                                                                                                              & 108.94                                                                                                & 19.14                                                                                                                              \\ \hline
100                                                                                             & 10                                                                                                                      & 8865.56                                                                                                                                    & 114.44                                                                                                                               & 90.53                                                                                                 & 1.18                                                                                                                               \\ \hline
1000                                                                                            & 1                                                                                                                       & 8972.90                                                                                                                                    & 390.40                                                                                                                               & 892.71                                                                                                & 39.87                                                                                                                              \\ \hline
1000                                                                                            & 10                                                                                                                      & 8972.47                                                                                                                                    & 443.07                                                                                                                               & 892.33                                                                                                & 44.76                                                                                                                              \\ \hline
\end{tabular}
    \centering
    \caption[Impact of parallel streams for \texttt{TCP} throughput]{Impact of parallel streams for \texttt{TCP} throughput. Here packets travel via an intermediate machine.}
    \label{fig:TCP_throughput_via_B_using_parallel_streams}
\end{table}




% TODO
\simon{TODO: comparison of TCP vs UDP}


    \begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{figs/TCP_throughput_in_time.png}
    \caption[Change of \texttt{TCP} throughput in time when a single \texttt{iperf3} client sends \texttt{TCP} traffic to the \texttt{iperf3} server via an intermediate machine]{Change of \texttt{TCP} throughput in time when a single \texttt{iperf3} client sends \texttt{TCP} traffic to the \texttt{iperf3} server via an intermediate machine. Here throughput is measured at the granularity of 0.1 s}
    \label{fig:TCP_throughput_in_time}
    \end{figure}
    
    
    
    
%\simon{Why did iperf3 report lower throughput for UDP? According to \url{https://fuchsia.googlesource.com/third_party/iperf/+/3.0.2/README.md}, iperf3 might not be the best tool for high UDP throughput measurements. The source recommends using another tool nuttcp instead }
    
% SERVER: ip netns exec mr_server nuttcp -S
% CLIENT: taskset -c 1 ip netns exec mr_client nuttcp  -l1448 -T30 -u -w40m -Ru -i1 10.2.2.101

\subsection{Throughput of Parallel \texttt{TCP} Streams that use \texttt{TLS}}
 
Throughput measurements of \texttt{TCP} with \texttt{TLS} are relevant because these values form an equal comparison with \texttt{QUIC} throughput.
In other words, as I have already mentioned in Section~\ref{QUIC_details_section}, \texttt{QUIC} uses cryptographic handshakes by default, meaning that for a fair comparison with \texttt{QUIC}, \texttt{TCP} also has to include \texttt{Transport Layer Security} layer.

Unfortunately, I could not measure the throughput of parallel \texttt{TCP} streams that use \texttt{TLS}.
It turns out that \texttt{iperf3}, the aforementioned throughput measurement tool, currently does not generate combined \texttt{TCP} and \texttt{TLS} throughput.
However, I found one extension of \texttt{iperf3} that claims to provide support for \texttt{TLS} \footnote{\url{https://github.com/Mic92/iperf-3.7}} but it had build errors so I did not use this extension.
In addition, I tried using \texttt{MsQuic} performance analysis tools that have explicit options to perform measurements with \texttt{TCP} rather than \texttt{QUIC}. 
But this functionality is not supported on Linux.
Unfortunately, I used \texttt{Ubuntu 20.04 LTS (Focal Fossa)} operating system on both machines in the testing environment (shown in Figure~\ref{fig:Physical_testing_environment}).
In the future, \texttt{MsQuic} tools may provide better support for the family of Unix operating systems meaning that subsequent studies could take throughput of \texttt{TCP} with \texttt{TLS} into account.

To evaluate \texttt{TCP with TLS} performance, I connected two laptops running on \texttt{Windows 10} OS with 1 Gigabit-per-second Ethernet link.
However, initially, firewalls on both machines prevented the successful packet exchange.
After turning firewalls off, I managed to exchange \texttt{ping} packets between the two computers.
However, because of the suspected additional antivirus rules, I could not run \texttt{MsQuic} performance analysis tools and measure \texttt{TCP with TLS} performance on Windows.
Actually, it was speculated that firewalls could be a noticeable \texttt{QUIC} deployment issue~\cite{The_QUIC_Transport_Protocol_Design_and_Internet-Scale_Deployment}.

\subsection{Throughput of Parallel \texttt{QUIC} Streams}

Table~\ref{fig:QUIC_throughput_via_B_using_parallel_requests} summarises the impact of  parallel \texttt{QUIC} connections for \texttt{QUIC's} throughput.
In particular, these experiments measured \texttt{QUIC} throughput between the encrypted server (i.e., \texttt{ngtcp2}) and client (i.e., \texttt{h2load}).

ngtcp2-h2load version
I transferred packets via the intermediate machine which was configured not to add any additional packet delay, loss or reordering.


% TODO
\simon{TODO: describe testing conditions in prose}

Number of tests for each experiment: 10


Uses 

GSO enabled



\begin{table}[H]
\begin{tabular}{|r|r|r|r|r|r|}
\hline
\multicolumn{1}{|c|}{\begin{tabular}[c]{@{}c@{}}Requested \\ file size \\ (MBytes)\end{tabular}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Number of \\ parallel \\ clients \\ (1 request \\ per client)\end{tabular}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Average\\ total \\ throughput \\ measured \\ on the \\ receiver's \\ side (MBit/s)\end{tabular}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Standard \\ deviation \\ of the \\ throughput \\ sample \\ (MBit/s)\end{tabular}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Average \\ total \\ completion \\ time (ms)\end{tabular}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Standard \\ deviation \\ of the\\ completion \\ time \\ sample (ms)\end{tabular}} \\ \hline
1                                                                                                & 1                                                                                                                          & 393.64                                                                                                                                          & 20.53                                                                                                                            & 20.38                                                                                                    & 1.07                                                                                                                             \\ \hline
1                                                                                                & 10                                                                                                                         & 594.66                                                                                                                                          & 4.40                                                                                                                             & 134.59                                                                                                   & 1.00                                                                                                                             \\ \hline
10                                                                                               & 1                                                                                                                          & 706.30                                                                                                                                          & 11.19                                                                                                                            & 113.33                                                                                                   & 1.82                                                                                                                             \\ \hline
10                                                                                               & 10                                                                                                                         & 766.00                                                                                                                                          & 2.94                                                                                                                             & 1044.00                                                                                                  & 5.16                                                                                                                             \\ \hline
100                                                                                              & 1                                                                                                                          & 767.72                                                                                                                                          & 3.39                                                                                                                             & 1042.00                                                                                                  & 4.22                                                                                                                             \\ \hline
100                                                                                              & 10                                                                                                                         & 788.40                                                                                                                                          & 2.33                                                                                                                             & 10150.00                                                                                                 & 29.44                                                                                                                            \\ \hline
1000                                                                                             & 1                                                                                                                          & 755.04                                                                                                                                          & 15.51                                                                                                                            & 10602.00                                                                                                 & 219.53                                                                                                                           \\ \hline
1000                                                                                             & 10                                                                                                                         & 685.84                                                                                                                                          & 7.55                                                                                                                             & 116694.00                                                                                                & 1258.09                                                                                                                          \\ \hline
\end{tabular}
    \centering
    \caption{Impact of parallel requests for encrypted \texttt{QUIC} throughput. Here packets travel via an intermediate machine. 
    \simon{h2load experiment is probably sending 1 or 10 separate "files" while iperf3 is probably sending just a single "file" over 1 or 10 connections}}
    \label{fig:QUIC_throughput_via_B_using_parallel_requests}
\end{table}
















\section{Impact of Cryptographic Operations}
% TODO:
% Ran experiments with different network conditions:
% > delay
% > uncorrelated packet loss -- meaning that the probability of loosing a packet does not increase if the previous packet is lost
% > (TODO) packet reordering
% > \simon{Why do we care about packet reordering? According to https://tools.ietf.org/html/draft-ietf-quic-transport-32, section 9.4, packet reordering is more likely to occur when the server changes its address and sends packets via multiple paths.

% conditions:
%  sampling frequency=997 (step locking)
%  MTU=default(1252)
%  number of samples



\subsection{Performance Comparison of encrypted and unencrypted versions of \texttt{ngtcp2}}




\begin{table}[H]
\begin{tabular}{|r|r|r|r|r|r|}
\hline
\multicolumn{1}{|c|}{\begin{tabular}[c]{@{}c@{}}Requested \\ file size \\ (MBytes)\end{tabular}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Number of \\ parallel \\ clients \\ (1 request \\ per client)\end{tabular}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Average\\ total \\ throughput \\ measured \\ on the \\ receiver's \\ side (MBit/s)\end{tabular}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Standard \\ deviation \\ of the \\ throughput \\ sample \\ (MBit/s)\end{tabular}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Average \\ total \\ completion \\ time (ms)\end{tabular}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Standard \\ deviation \\ of the\\ completion \\ time \\ sample (ms)\end{tabular}} \\ \hline
1                                                                                                & 1                                                                                                                          & 713.35                                                                                                                                          & 44.04                                                                                                                            & 11.26                                                                                                    & 0.70                                                                                                                             \\ \hline
1                                                                                                & 10                                                                                                                         & 1325.47                                                                                                                                         & 12.02                                                                                                                            & 60.39                                                                                                    & 0.55                                                                                                                             \\ \hline
10                                                                                               & 1                                                                                                                          & 2092.74                                                                                                                                         & 19.25                                                                                                                            & 38.24                                                                                                    & 0.35                                                                                                                             \\ \hline
10                                                                                               & 10                                                                                                                         & 2411.86                                                                                                                                         & 9.94                                                                                                                             & 331.81                                                                                                   & 1.37                                                                                                                             \\ \hline
100                                                                                              & 1                                                                                                                          & 2579.74                                                                                                                                         & 20.39                                                                                                                            & 310.22                                                                                                   & 2.48                                                                                                                             \\ \hline
100                                                                                              & 10                                                                                                                         & 2642.97                                                                                                                                         & 27.40                                                                                                                            & 3029.00                                                                                                  & 31.07                                                                                                                            \\ \hline
1000                                                                                             & 1                                                                                                                          & 2661.93                                                                                                                                         & 7.01                                                                                                                             & 3006.00                                                                                                  & 8.43                                                                                                                             \\ \hline
1000                                                                                             & 10                                                                                                                         & 2557.58                                                                                                                                         & 73.76                                                                                                                            & 31312.00                                                                                                 & 891.61                                                                                                                           \\ \hline
\end{tabular}
    \centering
    \caption[Impact of parallel requests for unencrypted \texttt{QUIC} throughput]{Impact of parallel requests for unencrypted \texttt{QUIC} throughput. Here packets travel via an intermediate machine}
    \label{fig:unencrypted_QUIC_throughput_via_B_using_parallel_requests}
\end{table}



\simon{1. Add a flame chart of ENCRYPTED ngtcp2 behaviour over some standard link and sufficiently large requested file size}

\simon{2. Add a flame chart of UNENCRYPTED ngtcp2 behaviour over the same link and sufficiently large requested file size}




\subsection{Performance Comparison of \textit{Null} encryption in \texttt{MsQuic} and \texttt{ngtcp2}}

As I have already mentioned, \texttt{MsQuic}, \texttt{Microsoft's} implementation of \texttt{QUIC}, has the pre-built option to disable cryptography.
\enquote{\textit{Null} encryption} mechanism of  \texttt{MsQuic} works as follows.
At first, communicating end hosts still have to complete a secure handshake~\cite{banks-quic-disable-encryption-00}.
After that, if both client and server express their will to turn off encryption, then plaintext packets are used for communications (i.e. both packet headers and payloads are unencrypted)~\cite{banks-quic-disable-encryption-00}.
However, according to Mr N. Banks, one drawback of continuing to use encryption for the first \texttt{QUIC} handshake is that performance of \texttt{QUIC} handshake is not improved.

\simon{Add a chart showing throughput of encrypted/encrypted ngtcp2 throughput + encrypted/encrypted MsQuic throughput}


\subsection{Performance Comparison of \textit{Null} Encryption in Application Layer}

\simon{Compare throughputs of HTTP/3 with null crypto vs HTTP/2 with "--no-tls-proto" (h2load client supports that, now I just need to find HTTP/2 server)}

\section{\texttt{QUIC} Performance Evaluation}


\subsection{Impact of Link Delay}

\simon{Add a graph that would demonstrate how throughput of unencrypted ngtcp2 changes with the increasing additional packet DELAY. No packet loss should be added. Probably I should use similar delays like the ones that were used in other papers (e.g. +10ms, +50ms, +100ms).}

\simon{What requested file size should I use for these measurements?}

\subsection{Impact of Packet Loss}

\simon{Add a graph that would demonstrate how throughput of unencrypted ngtcp2 changes with the increasing additional packet LOSS. }

\simon{Use packet loss: 1\%, 0.1\%, 0.01\%, 0.001\%}

\simon{In addition, these loss values should be equal to the ones used in MsQuic performance dashboard}

\simon{What requested file size should I use for these measurements?}





\section{Final the Most Optimised Version Of \texttt{QUIC}}

\simon{It might be an interesting section to combine all of the performance-boosting ideas (GSO, null-crypto, Jumboframes, large frame sizes) to create an ngtcp2 version with the highest throughput}






\chapter{Conclusion}
% ------------------------------------------------
%This chapter is likely to be very short and it may well refer back to the Introduction. It might offer a reflection on the lessons learned and explain how you would have planned the project if starting again with the benefit of hindsight.


% FROM: https://www.cst.cam.ac.uk/teaching/part-ii/projects/assessment
%Clearly presented argument demonstrating success criteria met.
%Good or excellent evidence of critical thought and interpretation of the results which substantiate any claims of success, improvements or novelty.
%Conclusions provide an effective summary of work completed along with good future work.
%Personal reflection on the lessons learned.
% ------------------------------------------------

\section{Summary}

In this project, I simulated offloading of cryptographic \texttt{QUIC} functions, and I measured the impact of this improvement.
My dissertation reiterates the idea that, if implemented correctly, hardware offloading of \texttt{QUIC} could offer substantial performance improvements.

\simon{Mention energy efficiency and application of QUIC for IoT https://calendar.perfplanet.com/2018/quic-and-http-3-too-big-to-fail/}

\section{Reflection}
Before starting the project, prior expertise working with the \texttt{NetFPGA} platform would have substantially helped.
In particular, hands-on experience with the reference hardware projects during the summer before Part II would have allowed me to start working on the implementation of the hardware offloading project during the Michaelmas term.


\section{Future Work}

Certain innovative network analysis tools, such as the aforementioned \texttt{MsQuic} performance analysis programs, were not adopted for \texttt{Ubuntu 20.04 LTS (Focal Fossa)} operating system.
Hence, I could not compare performance of disabled encryption of \texttt{MsQuic} with the corresponding \texttt{TCP/TLS} stack.
However, I hope that in the future, such experiments would be possible.

% TODO
\simon{TODO: suggest offloading crypto to hardware (as Gianni's team has already suggested)}

\simon{TODO: critique that offloading \texttt{QUIC} to hardware might stifle innovation in the long term. }

\simon{TODO: for example, look into invariants of \texttt{QUIC} and check if such offloading would be acceptable}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% the bibliography
\addcontentsline{toc}{chapter}{Bibliography}
\bibliography{refs}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%TC:ignore
% the appendices
\appendix

\chapter{Preparation script from \texttt{\textasciitilde/.zshrc}} \label{preparation_script_from_zshrc}

\begin{verbatim}
# ----------------------------------------------------

echo "0. Running ~/.zshrc"

echo "Setting up go environment"
export PATH=$PATH:/usr/local/go/bin

# ...

# echo "2. Setting X11-forwarding"
# xauth add $(xauth -f ~user109/.Xauthority list | tail -1)

echo "3. List of available ports"
ifconfig -a

# ...

echo "6.  Waking up network interfaces:"
echo "6.0 Deleting old assignment of network interfaces"
ip netns delete mr_client
ip netns delete mr_server
ip netns add mr_client
ip netns add mr_server

echo "6.1 Initial configuration:"
sleep 4
ip link set eth2 up
ip link set eth3 up
ip link set nf0 up
ip link set nf1 up
ip link set nf2 up
ip link set nf3 up
sleep 4

export client_subnet="10.1.1"
export server_subnet="10.2.2"
export client_IP=${client_subnet}".100"
export server_IP=${server_subnet}".101"
export client_default_gateway_IP=${client_subnet}".1"
export server_default_gateway_IP=${server_subnet}".1"

echo "client_IP: $client_IP"
echo "server_IP: $server_IP"
echo "client_default_gateway_IP: $client_default_gateway_IP"
echo "server_default_gateway_IP: $server_default_gateway_IP"

echo "6.2 Setting IP addresses for nf0, nf1, eth2, eth3"
ifconfig nf0  10.4.4.100 netmask 255.255.255.0
ifconfig nf1  10.4.4.101 netmask 255.255.255.0
ifconfig nf2  10.4.4.102 netmask 255.255.255.0
ifconfig nf3  10.4.4.103 netmask 255.255.255.0
ifconfig eth2 $client_IP netmask 255.255.255.0
ifconfig eth3 $server_IP netmask 255.255.255.0

ip link set dev eth2 mtu 1500
ip link set dev eth3 mtu 1500

ifconfig -a


echo "6.3 Setting virtual namespaces"
ip link set dev eth2 netns mr_client
ip link set dev eth3 netns mr_server
ip netns exec mr_client ip addr add $client_IP/24 dev eth2
ip netns exec mr_server ip addr add $server_IP/24 dev eth3
ip netns exec mr_client ip link set dev eth2 up
ip netns exec mr_server ip link set dev eth3 up
sleep 2
ip netns exec mr_client route add default gw $client_default_gateway_IP eth2
ip netns exec mr_server route add default gw $server_default_gateway_IP eth3

ip netns exec mr_client ip route show
ip netns exec mr_server ip route show

echo "6.3.1 Configuration of network namespaces"
ip netns exec mr_client ip addr list | grep "eth2"
ip netns exec mr_server ip addr list | grep "eth3"
echo "\n"

ip netns exec mr_client ping -c1 "$server_IP" | grep "received"
ip netns exec mr_client ping -c1 "$client_default_gateway_IP" | grep "received"
ip netns exec mr_server ping -c1 "$client_IP" | grep "received"
ip netns exec mr_server ping -c1 "$server_default_gateway_IP" | grep "received"

echo "\n"


cd /root/evaluation/Part-II-dissertation

echo "8. Setting SSLKEYLOGFILE"
export SSLKEYLOGFILE=/root/evaluation/unencrypted_stack/ngtcp2/examples/server.key
echo " SSLKEYLOGFILE: $SSLKEYLOGFILE"

echo "9. Turn off hyperthreading"
echo off > /sys/devices/system/cpu/smt/control
lscpu | grep "per core"

# ...

echo "11. checking if offload is turned off..."
echo "  CLIENT:"
ip netns exec mr_client ethtool -K eth2 lro off
ip netns exec mr_client ethtool -K eth2 gso off
ip netns exec mr_client ethtool -k eth2 | grep large-receive-offload
ip netns exec mr_client ethtool -k eth2 | grep tcp-segmentation-offload

echo "  SERVER:"
ip netns exec mr_server ethtool -K eth3 lro off
ip netns exec mr_server ethtool -K eth3 gso off
ip netns exec mr_server ethtool -k eth3 | grep large-receive-offload
ip netns exec mr_server ethtool -k eth3 | grep tcp-segmentation-offload

#
# ----------------------------------------------------

\end{verbatim}

\chapter{Source B}

    \section{GSO impact measurement results}
\begin{table}[H]
\begin{tabular}{|l|l|r|r|r|r|r|r|}
\hline
\multicolumn{1}{|c|}{\begin{tabular}[c]{@{}c@{}}number \\ of \\ experiments:\end{tabular}} & \multicolumn{1}{c|}{10}                                                                       & \multicolumn{1}{c|}{}                                                                          & \multicolumn{1}{c|}{}                                                         & \multicolumn{1}{c|}{}                                                                           & \multicolumn{1}{c|}{}                                                                                         & \multicolumn{1}{c|}{}                                                                         & \multicolumn{1}{c|}{}                                                                                       \\ \hline
\multicolumn{1}{|c|}{\begin{tabular}[c]{@{}c@{}}delay\\ (ms)\end{tabular}}                 & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}probability\\ of\\ packet\\ loss\end{tabular}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Requested\\ file\\ size\\ (bytes)\end{tabular}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Is GSO\\ enabled\end{tabular}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}average\\ completion\\ time\\ (ms)\end{tabular}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}standard \\ deviation\\ completion\\ time\\ (ms)\end{tabular}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}average\\ throughput\\ (Mbits/s)\end{tabular}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}standard \\ deviation\\ throughput\\ (Mbits/s)\end{tabular}} \\ \hline
0                                                                                          & 0                                                                                             & 1000000                                                                                        & no GSO                                                                        & 27.54                                                                                           & 0.41                                                                                                          & 290.62                                                                                        & 4.29                                                                                                        \\ \hline
0                                                                                          & 0                                                                                             & 1000000                                                                                        & with GSO                                                                      & 21.27                                                                                           & 1.40                                                                                                          & 377.69                                                                                        & 23.06                                                                                                       \\ \hline
0                                                                                          & 0                                                                                             & 10000000                                                                                       & no GSO                                                                        & 201.87                                                                                          & 2.30                                                                                                          & 396.46                                                                                        & 4.52                                                                                                        \\ \hline
0                                                                                          & 0                                                                                             & 10000000                                                                                       & with GSO                                                                      & 133.65                                                                                          & 2.53                                                                                                          & 598.97                                                                                        & 11.15                                                                                                       \\ \hline
0                                                                                          & 0                                                                                             & 100000000                                                                                      & no GSO                                                                        & 1,897.00                                                                                        & 24.06                                                                                                         & 421.96                                                                                        & 5.27                                                                                                        \\ \hline
0                                                                                          & 0                                                                                             & 100000000                                                                                      & with GSO                                                                      & 1,218.00                                                                                        & 4.22                                                                                                          & 657.49                                                                                        & 1.74                                                                                                        \\ \hline
0                                                                                          & 0                                                                                             & 1000000000                                                                                     & no GSO                                                                        & 19,125.00                                                                                       & 145.47                                                                                                        & 418.44                                                                                        & 3.21                                                                                                        \\ \hline
0                                                                                          & 0                                                                                             & 1000000000                                                                                     & with GSO                                                                      & 12,068.00                                                                                       & 17.51                                                                                                         & 663.06                                                                                        & 0.98                                                                                                        \\ \hline
\end{tabular}
    \centering
    \caption{GSO impact measurement results}
    \label{GSO_impact_measurement_results}
\end{table}

\chapter{Project Proposal}
\input{proposal}
%TC:endignore
 \end{document}
